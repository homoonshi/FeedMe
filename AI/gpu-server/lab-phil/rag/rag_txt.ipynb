{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "GiQs8-_Nn_Ml"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q langchain pypdf sentence-transformers chromadb openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VgYrF6FbNkJL",
    "outputId": "825dd743-7700-45c3-dee2-5f6e79ada8b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.10\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_HBhuL23S_YR"
   },
   "source": [
    "## Multi-Query Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Iterator\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.document_loaders.helpers import detect_file_encodings\n",
    "import pandas as pd\n",
    "from langchain_community.document_loaders import CSVLoader\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "class KaKaoTalkLoader(CSVLoader):\n",
    "    def __init__(self, file_path: str, file_suffix:str, encoding: str = \"utf8\", **kwargs):\n",
    "        super().__init__(file_path, encoding=encoding, **kwargs)\n",
    "        # NOTE - choh(2024.04.05) - íŒŒì¼ í™•ì¥ì ë³€ìˆ˜ ì¶”ê°€\n",
    "        self.file_suffix = file_suffix\n",
    "    \n",
    "    def anonymize_user_id(self, user_id, num_chars_to_anonymize=3):\n",
    "        \"\"\"\n",
    "        ë¹„ì‹ë³„í™” í•¨ìˆ˜ëŠ” ì£¼ì–´ì§„ ì‚¬ìš©ì IDì˜ ì•ë¶€ë¶„ì„ '*'ë¡œ ëŒ€ì²´í•˜ì—¬ ë¹„ì‹ë³„í™”í•©ë‹ˆë‹¤.\n",
    "\n",
    "        :param user_id: ë¹„ì‹ë³„í™”í•  ì‚¬ìš©ì ID\n",
    "        :param num_chars_to_anonymize: ë¹„ì‹ë³„í™”í•  ë¬¸ì ìˆ˜\n",
    "        :return: ë¹„ì‹ë³„í™”ëœ ì‚¬ìš©ì ID\n",
    "        \"\"\"\n",
    "        # ë¹„ì‹ë³„í™”í•  ë¬¸ì ìˆ˜ê°€ ì‚¬ìš©ì IDì˜ ê¸¸ì´ë³´ë‹¤ ê¸¸ ê²½ìš°, ì „ì²´ IDë¥¼ '*'ë¡œ ëŒ€ì²´\n",
    "        if num_chars_to_anonymize >= len(user_id):\n",
    "            num_chars_to_anonymize = len(user_id) - 1\n",
    "            return \"*\" * num_chars_to_anonymize\n",
    "\n",
    "        # ì•ë¶€ë¶„ì„ '*'ë¡œ ëŒ€ì²´í•˜ê³  ë‚˜ë¨¸ì§€ ë¶€ë¶„ì„ ì›ë³¸ IDì—ì„œ ê°€ì ¸ì˜´\n",
    "        anonymized_id = \"*\" * num_chars_to_anonymize + user_id[num_chars_to_anonymize:]\n",
    "\n",
    "        return anonymized_id\n",
    "    \n",
    "    # NOTE - choh(2024.04.05) - 12ì‹œê°„ì œë¥¼ 24ì‹œê°„ì œë¡œ ë³€í™˜\n",
    "    def process_time_to_24hr_format(self, date_obj, time_str):\n",
    "        \"\"\"\n",
    "        ëŒ€í™” ë‚´ìš©ì¤‘ì— ì‹œê°„ í‘œì‹œê°€ 'ì˜¤ì „ 12:23', 'ì˜¤í›„ 11:23'ê³¼ ê°™ì´ 12ì‹œê°„ì œë¡œ ë˜ì–´ ìˆëŠ” ê²½ìš°, \n",
    "        ì´ë¥¼ 24ì‹œê°„ì œë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
    "        \n",
    "        :param date_obj: ëŒ€í™” ë‚´ìš©ì˜ ë‚ ì§œ ì •ë³´ê°€ ë‹´ê¸´ datetime ê°ì²´\n",
    "        :praam time_str: ëŒ€í™” ë‚´ìš©ì˜ ì‹œê°„ ì •ë³´ê°€ ë‹´ê¸´ ë¬¸ìì—´\n",
    "        :return: 24ì‹œê°„ì œë¡œ ë³€í™˜ëœ datetime ê°ì²´\n",
    "        \"\"\"\n",
    "        \n",
    "        # 'ì˜¤ì „/ì˜¤í›„' ë¶€ë¶„ê³¼ ì‹œê°„ ë¶€ë¶„ì„ ë¶„ë¦¬í•©ë‹ˆë‹¤.\n",
    "        period, time_part = time_str.split(' ', 1)\n",
    "        \n",
    "        # ì‹œê°„ ë¶€ë¶„ì„ ì‹œì™€ ë¶„ìœ¼ë¡œ ë‹¤ì‹œ ë¶„ë¦¬í•©ë‹ˆë‹¤.\n",
    "        hour, minute = map(int, time_part.split(':'))\n",
    "        \n",
    "        # 'ì˜¤í›„'ì¸ ê²½ìš° 12ë¥¼ ë”í•˜ë˜, 'ì˜¤í›„ 12ì‹œ'ëŠ” ì œì™¸í•©ë‹ˆë‹¤.\n",
    "        if period == 'ì˜¤í›„' and hour != 12:\n",
    "            hour += 12\n",
    "        # 'ì˜¤ì „ 12ì‹œ'ëŠ” 0ì‹œë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.\n",
    "        elif period == 'ì˜¤ì „' and hour == 12:\n",
    "            hour = 0\n",
    "        \n",
    "        # date_objê³¼ ê²°í•©í•˜ì—¬ ìµœì¢… datetime ê°ì²´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "        # ì—¬ê¸°ì„œ datetime í•¨ìˆ˜ëŠ” ìœ„ì—ì„œ ì„í¬íŠ¸í•œ datetime í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "        combined_datetime = datetime(date_obj.year, date_obj.month, date_obj.day, hour, minute)\n",
    "        \n",
    "        # pandasì˜ to_datetime í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ pandas.Timestamp ê°ì²´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
    "        return pd.to_datetime(combined_datetime)\n",
    "    \n",
    "    # NOTE - choh(2024.04.05) - ëŒ€í™”ëª©ë¡ì˜ ë‚ ì§œ ë³€í™˜ ë¶€ë¶„ì„ íŒŒì‹±\n",
    "    def process_date(self, line: str) -> tuple:\n",
    "        \"\"\"\n",
    "        -------- 2024ë…„ 4ì›” 5ì¼ í™”ìš”ì¼ -------- í˜•íƒœì˜ ë‚ ì§œë¥¼ íŒŒì‹±í•˜ê³ ,\n",
    "        íŒŒì‹± ì„±ê³µ ì—¬ë¶€ì™€ í•¨ê»˜ íŒŒì‹±ëœ ë‚ ì§œ ë˜ëŠ” ì›ë˜ ë¬¸ìì—´ì„ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "        \n",
    "        :param line: ë‚ ì§œ ë¬¸ìì—´\n",
    "        :return: (íŒŒì‹± ì„±ê³µ ì—¬ë¶€, íŒŒì‹±ëœ ë‚ ì§œ ë˜ëŠ” ì›ë˜ ë¬¸ìì—´)\n",
    "        \"\"\"\n",
    "        # -------- 2024ë…„ 4ì›” 5ì¼ í™”ìš”ì¼ -------- ë‚ ì§œê°€ ì´ìƒíƒœì„\n",
    "        date_match = re.match(r'[-]+ (\\d+ë…„ \\d+ì›” \\d+ì¼) [^\\d]+', line)\n",
    "        if date_match:\n",
    "            # 2024ë…„ 4ì›” 5ì¼, í˜•íƒœì˜ ë‚ ì§œ ì¶”ì¶œ\n",
    "            date_pattern = re.compile(r'(\\d+)ë…„ (\\d+)ì›” (\\d+)ì¼')\n",
    "            match = date_pattern.search(date_match.group(1))\n",
    "            if match:\n",
    "                year, month, day = map(int, match.groups())\n",
    "                return (True, pd.to_datetime(f\"{year}-{month}-{day}\"))\n",
    "        return (False, line)\n",
    "\n",
    "    # NOTE - choh(2024.04.05) - __read_fileì„ í…ŒìŠ¤íŠ¸ í•˜ê¸° ìœ„í•œ wrapper í•¨ìˆ˜\n",
    "    def _read_file_test(self, csvfile) -> Iterator[Document]:\n",
    "        \"\"\"í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ ë˜í¼ í•¨ìˆ˜\"\"\"\n",
    "        return self.__read_file(csvfile)\n",
    "    \n",
    "    def __read_file(self, csvfile) -> Iterator[Document]:\n",
    "        # NOTE - choh(2024.04.05) - TXT í˜•íƒœì˜ ëŒ€í™” ë©”ì„¸ì§€ ì‚¬ì „ ì²˜ë¦¬\n",
    "        if self.file_suffix == \".txt\":\n",
    "            \n",
    "            # ì „ë‚  ë‚ ì§œ ë³€ìˆ˜ ì´ˆê¸°í™”\n",
    "            temp_date = None\n",
    "            i = 0 # í–‰ ë²ˆí˜¸\n",
    "            for line in csvfile:\n",
    "                \n",
    "                # ì´ë²ˆ ì¤„ì´ ë‚ ì§œê°€ ë§ìœ¼ë©´ is_parsed=True, resultëŠ” ë‚ ì§œ\n",
    "                is_parsed, result = self.process_date(line)\n",
    "                \n",
    "                # íŒŒì‹±í•œ ë¬¸ìì—´ì´ ë‚ ì§œ íŒ¨í„´ì— ë§ìœ¼ë©´, ë‚ ì§œë¥¼ ì €ì¥\n",
    "                if is_parsed:\n",
    "                    temp_date = result\n",
    "                \n",
    "                # ë‚ ì§œê°€ ì•„ë‹ˆë©´, ì²´íŒ…ì´ê¸° ë•Œë¬¸ì—, ì²´íŒ…ì„ íŒ¨í„´ ë§¤ì¹­\n",
    "                else:\n",
    "                    # ì´ˆê¸°ê°’ ì„¤ì •\n",
    "                    user = None\n",
    "                    time_12hr = None\n",
    "                    message = None\n",
    "\n",
    "                    # ëŒ€í™” íŒ¨í„´ ì°¾ê¸°\n",
    "                    conversation_match = re.match(r'\\[([^\\]]+)\\] \\[([^\\]]+)\\] (.+)', line)\n",
    "                    if conversation_match:\n",
    "                        user_real = conversation_match.group(1)\n",
    "                        time_12hr = conversation_match.group(2)\n",
    "                        message = conversation_match.group(3).strip()\n",
    "                        \n",
    "                        # ì‹œê°„ì„ 24ì‹œê°„ì œë¡œ ë³€í™˜                        \n",
    "                        date = self.process_time_to_24hr_format(temp_date, time_12hr)\n",
    "                        # ì‚¬ìš©ì ID ë¹„ì‹ë³„í™”\n",
    "                        user = self.anonymize_user_id(user_real)\n",
    "                        \n",
    "                        content = f'\"User: {user}, Message: {message}'\n",
    "                        \n",
    "                        metadata = {\n",
    "                            \"date\":  date.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                            \"year\": date.year,\n",
    "                            \"month\": date.month,\n",
    "                            \"day\": date.day,\n",
    "                            \"user\": user,\n",
    "                            \"row\": i,\n",
    "                            \"source\": str(self.file_path),\n",
    "                        }\n",
    "                        i += 1 # í–‰ ë²ˆí˜¸ ì¦ê°€\n",
    "                        yield Document(page_content=content, metadata=metadata)\n",
    "       \n",
    "        \n",
    "        # NOTE - choh(2024.04.05) - ê¸°ì¡´ ì½”ë“œ, csv íŒŒì¼ì¸ ê²½ìš°\n",
    "        else:\n",
    "            df = pd.read_csv(csvfile)\n",
    "            df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "            df[\"Date_strf\"] = df[\"Date\"].dt.strftime(\"%Y-%m-%d %H:%M:%S\").astype(str)\n",
    "            for i, row in df.iterrows():\n",
    "                date = row[\"Date\"]\n",
    "                user = self.anonymize_user_id(row[\"User\"])\n",
    "                content = f'\"User: {user}, Message: {row[\"Message\"]}'\n",
    "\n",
    "                metadata = {\n",
    "                    \"date\": row[\"Date_strf\"],\n",
    "                    \"year\": date.year,\n",
    "                    \"month\": date.month,\n",
    "                    \"day\": date.day,\n",
    "                    \"user\": user,\n",
    "                    \"row\": i,\n",
    "                    \"source\": str(self.file_path),\n",
    "                }\n",
    "                yield Document(page_content=content, metadata=metadata)\n",
    "\n",
    "    def lazy_load(self) -> Iterator[Document]:\n",
    "        try:\n",
    "            with open(self.file_path, newline=\"\", encoding=self.encoding) as csvfile:\n",
    "                yield from self.__read_file(csvfile)\n",
    "      \n",
    "        except UnicodeDecodeError as e:\n",
    "            if self.autodetect_encoding:\n",
    "                detected_encodings = detect_file_encodings(self.file_path)\n",
    "                for encoding in detected_encodings:\n",
    "                    try:\n",
    "                        with open(\n",
    "                            self.file_path, newline=\"\", encoding=encoding.encoding\n",
    "                        ) as csvfile:\n",
    "                            yield from self.__read_file(csvfile)\n",
    "                            break\n",
    "                    except UnicodeDecodeError:\n",
    "                        continue\n",
    "            else:\n",
    "                raise RuntimeError(f\"Error loading {self.file_path}\") from e\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error loading {self.file_path}\") from e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Y3TWw12cSqRw"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n",
      "/home/j-i11b104/.local/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n",
      "/home/j-i11b104/.local/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "# Build a sample vectorDB\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "import kakaotalk_loader as kakao\n",
    "\n",
    "file_path = \"KakaoTalk_group.txt\"\n",
    "file_suffix = \".txt\"  # Change to \".csv\" if you're using a CSV file\n",
    "loader = kakao.KaKaoTalkLoader(file_path, file_suffix, encoding=\"utf8\")\n",
    "\n",
    "# Load blog post\n",
    "loader = WebBaseLoader(\"https://corin-e.tistory.com/entry/1-%EC%82%BC%EC%84%B1-%EC%B2%AD%EB%85%84-SW-%EC%95%84%EC%B9%B4%EB%8D%B0%EB%AF%B8%EC%8B%B8%ED%94%BC-SSAFY-%EC%82%BC%EC%88%98-%ED%9B%84%EA%B8%B0\")\n",
    "data = loader.load()\n",
    "\n",
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=30)\n",
    "splits = text_splitter.split_documents(data)\n",
    "\n",
    "# VectorDB\n",
    "model_name = \"jhgan/ko-sbert-nli\"\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "ko_embedding = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "# sk-proj-5g3W1SVHGnmBB7Y2HEMCT3BlbkFJUSvDENpVaFEdRNwZHWlH\n",
    "# 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\n",
    "open_key = \"sk-proj-5g3W1SVHGnmBB7Y2HEMCT3BlbkFJUSvDENpVaFEdRNwZHWlH\"\n",
    "vectordb = Chroma.from_documents(documents=splits, embedding=ko_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "epoTy2FNSuqz"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/j-i11b104/.local/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "question = \"ì‘ì„±ìì˜ ìƒê°\"\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, openai_api_key = open_key)\n",
    "retriever_from_llm = MultiQueryRetriever.from_llm(\n",
    "    retriever=vectordb.as_retriever(), llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "eU6X_HctSumR"
   },
   "outputs": [],
   "source": [
    "# Set logging for the queries\n",
    "import logging\n",
    "\n",
    "logging.basicConfig()\n",
    "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "pXYmox2MSuZ-"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/j-i11b104/.local/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "INFO:langchain.retrievers.multi_query:Generated queries: ['1. What are the thoughts of the author?', \"2. What is the author's perspective?\", \"3. Can you provide insights into the author's mindset?\"]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_docs = retriever_from_llm.get_relevant_documents(query=question)\n",
    "len(unique_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "MIz2l4OaS75w"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'description': 'ê°€ë“œë‹ˆì˜ ë¸”ë¡œê·¸', 'language': 'ko', 'source': 'https://corin-e.tistory.com/entry/1-%EC%82%BC%EC%84%B1-%EC%B2%AD%EB%85%84-SW-%EC%95%84%EC%B9%B4%EB%8D%B0%EB%AF%B8%EC%8B%B8%ED%94%BC-SSAFY-%EC%82%BC%EC%88%98-%ED%9B%84%EA%B8%B0', 'title': '1) ì‚¼ì„± ì²­ë…„ SW ì•„ì¹´ë°ë¯¸(ì‹¸í”¼, SSAFY) ì‚¼ìˆ˜ í›„ê¸°'}, page_content='ì•ˆë…•í•˜ì„¸ìš”! ì •ë³´ ê³µìœ  í•´ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤.\\ní˜¹ì‹œ ë©´ì ‘ì€ ì£¼ë§ì— ë³´ì…¨ì„ê¹Œìš”?ã…œã…œ\\në°ë°ìˆ˜\\n\\n\\n\\n\\nì¸ë±ìŠ¤ : 0  1  2  3  4  5  6  7  8  9 10\\në°©  í–¥ :    ì™¼ â‹¯\\nì½”ë”©ì§ˆë¬¸ì\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCopyright Â© Kakao Corp. All rights reserved.\\n\\n\\n\\nê´€ë ¨ì‚¬ì´íŠ¸\\n\\n\\n\\n\\n\\n\\n\\n\\n\\ní‹°ìŠ¤í† ë¦¬íˆ´ë°”\\nğŸª¨ ğŸª¨ ğŸª¨êµ¬ë…í•˜ê¸°'),\n",
       " Document(metadata={'description': 'ê°€ë“œë‹ˆì˜ ë¸”ë¡œê·¸', 'language': 'ko', 'source': 'https://corin-e.tistory.com/entry/1-%EC%82%BC%EC%84%B1-%EC%B2%AD%EB%85%84-SW-%EC%95%84%EC%B9%B4%EB%8D%B0%EB%AF%B8%EC%8B%B8%ED%94%BC-SSAFY-%EC%82%BC%EC%88%98-%ED%9B%84%EA%B8%B0', 'title': '1) ì‚¼ì„± ì²­ë…„ SW ì•„ì¹´ë°ë¯¸(ì‹¸í”¼, SSAFY) ì‚¼ìˆ˜ í›„ê¸°'}, page_content='ê°ì¢… í›„ê¸° (2) \\n\\n\\n\\n\\n\\n\\n\\nìµœê·¼ê¸€ê³¼ ì¸ê¸°ê¸€\\n\\nìµœê·¼ê¸€\\nì¸ê¸°ê¸€\\n\\n\\n\\n\\n\\n\\nìŠ¤í”„ë§ + MongoDB) @JsonProperty @JsonNaming ì¸ì‹ ì•ˆë¨\\n2023.07.13 22:01\\n\\n\\n\\n\\n\\n\\n\\nERESOLVE unable to resolve dependency tree - npm install ì˜¤ë¥˜\\n2023.03.11 23:48\\n\\n\\n\\n\\n\\n\\ní”„ë¡œê·¸ë˜ë¨¸ìŠ¤ ìŠ¤í”„ë§ ì½”ë”©(2023ë…„ 2ì›”) í›„ê¸°\\n2023.02.11 12:18\\n\\n\\n\\n\\n\\n\\n\\n\\nì‹ ì… IT ê°œë°œì ë©´ì ‘ ì§ˆë¬¸ ì´ ì •ë¦¬ - ì¸ì„±/íšŒì‚¬/ì§ë¬´/ê²½í—˜/ê¸°ìˆ \\n2022.12.12 20:32\\n\\n\\n\\n\\n\\n\\n\\nì‹ ì… IT ê°œë°œì ì½”ë”© í…ŒìŠ¤íŠ¸ í›„ê¸° - ì–¸ì–´, ê³µë¶€ ë°©ë²•, ë‚œì´ë„ ë“±\\n2022.12.21 01:37\\n\\n\\n\\n\\n\\n\\n\\n1) ì‚¼ì„± ì²­ë…„ SW ì•„ì¹´ë°ë¯¸(ì‹¸í”¼, SSAFY) ì‚¼ìˆ˜ í›„ê¸°\\n2022.12.21 20:30\\n\\n\\n\\n\\n\\n\\n\\nìµœê·¼ëŒ“ê¸€\\n\\n\\n\\nì •ë§ ë„ì›€ì´ ë§ì´ ë˜ì—ˆìŠµë‹ˆë‹¤. ë³µ ë°›ìœ¼ì„¸ìš” ~!  ê°ì‚¬í•©ë‹ˆë‹¤. :)\\ní¬ë¡œë¡œ'),\n",
       " Document(metadata={'description': 'ê°€ë“œë‹ˆì˜ ë¸”ë¡œê·¸', 'language': 'ko', 'source': 'https://corin-e.tistory.com/entry/1-%EC%82%BC%EC%84%B1-%EC%B2%AD%EB%85%84-SW-%EC%95%84%EC%B9%B4%EB%8D%B0%EB%AF%B8%EC%8B%B8%ED%94%BC-SSAFY-%EC%82%BC%EC%88%98-%ED%9B%84%EA%B8%B0', 'title': '1) ì‚¼ì„± ì²­ë…„ SW ì•„ì¹´ë°ë¯¸(ì‹¸í”¼, SSAFY) ì‚¼ìˆ˜ í›„ê¸°'}, page_content='ê´€ë ¨ê¸€\\n\\n\\n\\n\\n\\n5-1) ë„¥í† ë¦¬ì–¼ ì§€ì› í›„ê¸° - ì½”ë”© í…ŒìŠ¤íŠ¸\\n2022.12.22\\n\\n\\n\\n\\n\\n\\n\\n4) NCì†Œí”„íŠ¸ ì‹ ì… ì§€ì› í›„ê¸° - ì½”ë”© í…ŒìŠ¤íŠ¸\\n2022.12.22\\n\\n\\n\\n\\n\\n\\n\\n3) ë„·ë§ˆë¸” ì»´í¼ë‹ˆ ì‹ ì… ì§€ì› í›„ê¸° - ì½”ë”© í…ŒìŠ¤íŠ¸\\n2022.12.22\\n\\n\\n\\n\\n\\n\\n\\n2) í”„ë¡œê·¸ë˜ë¨¸ìŠ¤ ë°±ì—”ë“œ ë°ë¸Œë§¤ì¹­/ìœˆí„°ì½”ë”© í›„ê¸° - ì½”ë”© í…ŒìŠ¤íŠ¸, ì„œë¥˜ ì „í˜•\\n2022.12.22\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nëŒ“ê¸€ 7\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\në¹„ë°€ê¸€\\n\\në“±ë¡\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nëŒëŒëŒ\\n\\n\\n\\n\\n\\n\\n\\në°©ë¬¸ììˆ˜Total\\n246,531\\n\\nToday : 89\\nYesterday : 173\\n\\n\\n\\n\\n\\n\\n ë¶„ë¥˜ ì „ì²´ë³´ê¸° (55) \\n JAVA (1) \\n JavaScript (1) \\n ë°±ì¤€ (21) \\n ìë£Œêµ¬ì¡° (2) \\n ê¸°íƒ€ (2) \\n ì˜¤ë¥˜ í•´ê²° (4) \\n ê°œë°œì ì·¨ì—… ì¤€ë¹„ (22) \\n ì¤€ë¹„ ë°©ë²• (2) \\n ì·¨ì¤€ í›„ê¸° (20) \\n\\n\\n ê°ì¢… í›„ê¸° (2)'),\n",
       " Document(metadata={'description': 'ê°€ë“œë‹ˆì˜ ë¸”ë¡œê·¸', 'language': 'ko', 'source': 'https://corin-e.tistory.com/entry/1-%EC%82%BC%EC%84%B1-%EC%B2%AD%EB%85%84-SW-%EC%95%84%EC%B9%B4%EB%8D%B0%EB%AF%B8%EC%8B%B8%ED%94%BC-SSAFY-%EC%82%BC%EC%88%98-%ED%9B%84%EA%B8%B0', 'title': '1) ì‚¼ì„± ì²­ë…„ SW ì•„ì¹´ë°ë¯¸(ì‹¸í”¼, SSAFY) ì‚¼ìˆ˜ í›„ê¸°'}, page_content=\"ê³µìœ í•˜ê¸°\\n\\nê²Œì‹œê¸€ ê´€ë¦¬\\n\\n\\nğŸª¨ ğŸª¨ ğŸª¨\\n\\n\\nì €ì‘ìí‘œì‹œ\\n\\n\\n\\n \\n\\n\\n\\n'ê°œë°œì ì·¨ì—… ì¤€ë¹„ > ì·¨ì¤€ í›„ê¸°' ì¹´í…Œê³ ë¦¬ì˜ ë‹¤ë¥¸ ê¸€\\n\\n\\n5-2) ë„¥í† ë¦¬ì–¼ ì§€ì› í›„ê¸° - ë©´ì ‘\\xa0\\xa0(1)\\n2022.12.22\\n\\n\\n5-1) ë„¥í† ë¦¬ì–¼ ì§€ì› í›„ê¸° - ì½”ë”© í…ŒìŠ¤íŠ¸\\xa0\\xa0(1)\\n2022.12.22\\n\\n\\n4) NCì†Œí”„íŠ¸ ì‹ ì… ì§€ì› í›„ê¸° - ì½”ë”© í…ŒìŠ¤íŠ¸\\xa0\\xa0(2)\\n2022.12.22\\n\\n\\n3) ë„·ë§ˆë¸” ì»´í¼ë‹ˆ ì‹ ì… ì§€ì› í›„ê¸° - ì½”ë”© í…ŒìŠ¤íŠ¸\\xa0\\xa0(0)\\n2022.12.22\\n\\n\\n2) í”„ë¡œê·¸ë˜ë¨¸ìŠ¤ ë°±ì—”ë“œ ë°ë¸Œë§¤ì¹­/ìœˆí„°ì½”ë”© í›„ê¸° - ì½”ë”© í…ŒìŠ¤íŠ¸, ì„œë¥˜ ì „í˜•\\xa0\\xa0(0)\\n2022.12.22\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTag\\nCT, IT, SSAFY, SW ì ì„± ì§„ë‹¨, ê°œë°œì, ë¹„ì „ê³µ, ì‚¼ì„±, ì‚¼ì„± ì²­ë…„ SW ì•„ì¹´ë°ë¯¸, ì‹¸í”¼\\n\\n\\n\\n'ê°œë°œì ì·¨ì—… ì¤€ë¹„/ì·¨ì¤€ í›„ê¸°'ì˜ ë‹¤ë¥¸ê¸€\\n\\n\\n\\ní˜„ì¬ê¸€1) ì‚¼ì„± ì²­ë…„ SW ì•„ì¹´ë°ë¯¸(ì‹¸í”¼, SSAFY) ì‚¼ìˆ˜ í›„ê¸°\\n\\në‹¤ìŒê¸€2) í”„ë¡œê·¸ë˜ë¨¸ìŠ¤ ë°±ì—”ë“œ ë°ë¸Œë§¤ì¹­/ìœˆí„°ì½”ë”© í›„ê¸° - ì½”ë”© í…ŒìŠ¤íŠ¸, ì„œë¥˜ ì „í˜•\\n\\n\\n\\n\\n\\nê´€ë ¨ê¸€\")]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "digNJMyRtM3k"
   },
   "source": [
    "## ê¸°ë³¸ Parent-document Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "id": "tZOk-3LOnb-0"
   },
   "outputs": [],
   "source": [
    "from langchain.retrievers import ParentDocumentRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "id": "f0ONC3wZn62e"
   },
   "outputs": [],
   "source": [
    "from langchain.storage import InMemoryStore\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "id": "VpJhZWThn7gG"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Maybe you meant '==' or ':=' instead of '='? (2736647683.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[123], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    loader = TextLoader('KakaoTalk_group.txt')\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax. Maybe you meant '==' or ':=' instead of '='?\n"
     ]
    }
   ],
   "source": [
    "loaders = [\n",
    "    loader = TextLoader('KakaoTalk_group.txt')\n",
    "    # PyPDFLoader(\"/content/drive/MyDrive/á„€á…¡á†¼á„‹á…´ á„Œá…¡á„…á…­/[á„‡á…©á†¨á„Œá…µá„‹á…µá„‰á…² FOCUS 15È£] á„€á…§á†¼á„€á…µá„ƒá…© á„€á…³á†¨á„Œá…¥á„‰á…µá†«á„‹á…­á†¼á„ƒá…¢á„á…®á†¯á„‰á…µá†·á„‰á…¡á„†á…©á„’á…§á†¼ á„€á…¢á„‡á…¡á†¯á„‹á…³á†¯ á„‹á…±á„’á…¡á†« á„€á…®á†¨á„‚á…¢ á„‰á…µá†«á„‹á…­á†¼á„Œá…¥á†¼á„‡á…© á„’á…ªá†¯á„‹á…­á†¼á„€á…¡á„‚á…³á†¼á„‰á…¥á†¼ á„á…¡á†·á„‰á…¢á†¨.pdf\"),\n",
    "    # PyPDFLoader(\"/content/drive/MyDrive/á„€á…¡á†¼á„‹á…´ á„Œá…¡á„…á…­/[á„‹á…µá„‰á…²á„…á…µá„‘á…©á„á…³ 2022-2á„’á…©] á„’á…§á†¨á„‰á…µá†«á„‰á…¥á†¼á„Œá…¡á†¼ á„Œá…¥á†¼á„á…¢á†¨á„€á…³á†·á„‹á…²á†¼ á„ƒá…©á†¼á„’á…£á†¼.pdf\"),\n",
    "]\n",
    "docs = []\n",
    "for loader in loaders:\n",
    "    docs.extend(loader.load_and_split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "id": "KBb-1ym5pKRG"
   },
   "outputs": [],
   "source": [
    "model_name = \"jhgan/ko-sbert-nli\"\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "ko_embedding = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "id": "fyixQa5SpHOW"
   },
   "outputs": [],
   "source": [
    "# This text splitter is used to create the child documents\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=200)\n",
    "# The vectorstore to use to index the child chunks\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"full_documents\", embedding_function=ko_embedding\n",
    ")\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryStore()\n",
    "retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "id": "IyVXzfq0pHME"
   },
   "outputs": [],
   "source": [
    "retriever.add_documents(docs, ids=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "id": "L8z2B2rKrAv3"
   },
   "outputs": [],
   "source": [
    "sub_docs = vectorstore.similarity_search(\"7ì›” 29ì¼ ëŒ€í™” ë‚´ìš©\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "id": "_axFoMvnrA-h"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ê¸€ ê¸¸ì´: 46\n",
      "\n",
      "\n",
      "1ë°˜ ì—ì´ìŠ¤ ë‹˜ê³¼ ì¹´ì¹´ì˜¤í†¡ ëŒ€í™”\n",
      "ì €ì¥í•œ ë‚ ì§œ : 2024-07-29 13:47:37\n"
     ]
    }
   ],
   "source": [
    "print(\"ê¸€ ê¸¸ì´: {}\\n\\n\".format(len(sub_docs[0].page_content)))\n",
    "print(sub_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "id": "W68G7ITorThj"
   },
   "outputs": [],
   "source": [
    "retrieved_docs = retriever.get_relevant_documents(\"7ì›” 29ì¼ ëŒ€í™” ë‚´ìš©\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "id": "Uzl334J8rXEQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ê¸€ ê¸¸ì´: 46\n",
      "\n",
      "\n",
      "1ë°˜ ì—ì´ìŠ¤ ë‹˜ê³¼ ì¹´ì¹´ì˜¤í†¡ ëŒ€í™”\n",
      "ì €ì¥í•œ ë‚ ì§œ : 2024-07-29 13:47:37\n"
     ]
    }
   ],
   "source": [
    "print(\"ê¸€ ê¸¸ì´: {}\\n\\n\".format(len(retrieved_docs[0].page_content)))\n",
    "print(retrieved_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AK752NU_tHjb"
   },
   "source": [
    "## ë³¸ë¬¸ì˜ Full_chunkê°€ ë„ˆë¬´ ê¸¸ë•Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "id": "q64Iw5Wbrd94"
   },
   "outputs": [],
   "source": [
    "# This text splitter is used to create the parent documents\n",
    "parent_splitter = RecursiveCharacterTextSplitter(chunk_size=800)\n",
    "# This text splitter is used to create the child documents\n",
    "# It should create documents smaller than the parent\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=200)\n",
    "# The vectorstore to use to index the child chunks\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"split_parents\", embedding_function=ko_embedding\n",
    ")\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryStore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "id": "1TVbBTmgr_CA"
   },
   "outputs": [],
   "source": [
    "retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    "    parent_splitter=parent_splitter,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "id": "K9bPu58hsAfP"
   },
   "outputs": [],
   "source": [
    "retriever.add_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "id": "KLvoTdeksUne"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(store.yield_keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "id": "K0Te96C7sVxA"
   },
   "outputs": [],
   "source": [
    "sub_docs = vectorstore.similarity_search(\"7ì›” 29ì¼ ëŒ€í™” ë‚´ìš©\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "id": "qJ6Qb_E_sZCb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1ë°˜ ì—ì´ìŠ¤ ë‹˜ê³¼ ì¹´ì¹´ì˜¤í†¡ ëŒ€í™”\n",
      "ì €ì¥í•œ ë‚ ì§œ : 2024-07-29 13:47:37\n"
     ]
    }
   ],
   "source": [
    "print(sub_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "id": "GXioc_E3so_U"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sub_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "id": "bL6F-aBJsZyv"
   },
   "outputs": [],
   "source": [
    "retrieved_docs = retriever.get_relevant_documents(\"7ì›” 29ì¼ ëŒ€í™” ë‚´ìš©\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "id": "EnhD6dY_sfKq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1ë°˜ ì—ì´ìŠ¤ ë‹˜ê³¼ ì¹´ì¹´ì˜¤í†¡ ëŒ€í™”\n",
      "ì €ì¥í•œ ë‚ ì§œ : 2024-07-29 13:47:37\n"
     ]
    }
   ],
   "source": [
    "print(retrieved_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "id": "UhxrZ1zAscVP"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(retrieved_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FoTL9eBctWva"
   },
   "source": [
    "## Self-querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "_DmHA3cgtZcJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting lark\n",
      "  Downloading lark-1.1.9-py3-none-any.whl.metadata (1.9 kB)\n",
      "Downloading lark-1.1.9-py3-none-any.whl (111 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m111.7/111.7 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: lark\n",
      "Successfully installed lark-1.1.9\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install lark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7TJLhLfQthCt"
   },
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "docs = [\n",
    "    Document(\n",
    "        page_content=\"A bunch of scientists bring back dinosaurs and mayhem breaks loose\",\n",
    "        metadata={\"year\": 1993, \"rating\": 7.7, \"genre\": \"science fiction\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Leo DiCaprio gets lost in a dream within a dream within a dream within a ...\",\n",
    "        metadata={\"year\": 2010, \"director\": \"Christopher Nolan\", \"rating\": 8.2},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea\",\n",
    "        metadata={\"year\": 2006, \"director\": \"Satoshi Kon\", \"rating\": 8.6},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"A bunch of normal-sized women are supremely wholesome and some men pine after them\",\n",
    "        metadata={\"year\": 2019, \"director\": \"Greta Gerwig\", \"rating\": 8.3},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Toys come alive and have a blast doing so\",\n",
    "        metadata={\"year\": 1995, \"genre\": \"animated\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Three men walk into the Zone, three men walk out of the Zone\",\n",
    "        metadata={\n",
    "            \"year\": 1979,\n",
    "            \"director\": \"Andrei Tarkovsky\",\n",
    "            \"genre\": \"thriller\",\n",
    "            \"rating\": 9.9,\n",
    "        },\n",
    "    ),\n",
    "]\n",
    "vectorstore = Chroma.from_documents(docs, ko_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9x2PsJ9e7tuH"
   },
   "outputs": [],
   "source": [
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "metadata_field_info = [\n",
    "    AttributeInfo(\n",
    "        name=\"genre\",\n",
    "        description=\"The genre of the movie. One of ['science fiction', 'comedy', 'drama', 'thriller', 'romance', 'action', 'animated']\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"year\",\n",
    "        description=\"The year the movie was released\",\n",
    "        type=\"integer\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"director\",\n",
    "        description=\"The name of the movie director\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"rating\", description=\"A 1-10 rating for the movie\", type=\"float\"\n",
    "    ),\n",
    "]\n",
    "document_content_description = \"Brief summary of a movie\"\n",
    "llm = ChatOpenAI(temperature=0, openai_api_key = \"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\")\n",
    "retriever = SelfQueryRetriever.from_llm(\n",
    "    llm,\n",
    "    vectorstore,\n",
    "    document_content_description,\n",
    "    metadata_field_info,\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "foYY4IEA74Dr"
   },
   "outputs": [],
   "source": [
    "retriever.get_relevant_documents(\"what are some movies rated higher than 8.5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zQP9iztA8ApB"
   },
   "source": [
    "## Time-weighted vector store Retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OKEps8vK8GIm"
   },
   "source": [
    "Scoring ë°©ë²• = *semantic_similarity + (1.0 - decay_rate) ^ hours_passed*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "zKL8WJ_P8UEG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q faiss-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "uLG48BJY8AEW"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'faiss'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datetime, timedelta\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfaiss\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocstore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InMemoryDocstore\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mretrievers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TimeWeightedVectorStoreRetriever\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'faiss'"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "import faiss\n",
    "from langchain.docstore import InMemoryDocstore\n",
    "from langchain.retrievers import TimeWeightedVectorStoreRetriever\n",
    "from langchain.schema import Document\n",
    "from langchain_community.vectorstores import FAISS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lv5TtUnK8Mml"
   },
   "outputs": [],
   "source": [
    "# Initialize the vectorstore as empty\n",
    "embedding_size = 768\n",
    "index = faiss.IndexFlatL2(embedding_size)\n",
    "vectorstore = FAISS(ko_embedding, index, InMemoryDocstore({}), {})\n",
    "retriever = TimeWeightedVectorStoreRetriever(\n",
    "    vectorstore=vectorstore, decay_rate=0.99, k=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vL-HP-9L8OIq"
   },
   "outputs": [],
   "source": [
    "yesterday = datetime.now() - timedelta(days=1)\n",
    "retriever.add_documents(\n",
    "    [Document(page_content=\"ì˜ì–´ëŠ” í›Œë¥­í•©ë‹ˆë‹¤.\", metadata={\"last_accessed_at\": yesterday})]\n",
    ")\n",
    "retriever.add_documents([Document(page_content=\"í•œêµ­ì–´ëŠ” í›Œë¥­í•©ë‹ˆë‹¤\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q6KCbXXV_d4F"
   },
   "outputs": [],
   "source": [
    "# \"Hello World\" is returned first because it is most salient, and the decay rate is close to 0., meaning it's still recent enough\n",
    "retriever.get_relevant_documents(\"ì˜ì–´ê°€ ì¢‹ì•„ìš”\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sjtkFgOGfemK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ltRw8HCmmECS"
   },
   "source": [
    "## Ensemble Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "QN9Qk5_LkeQH"
   },
   "outputs": [],
   "source": [
    "!pip install -q langchain pypdf sentence-transformers chromadb langchain-openai faiss-gpu --upgrade --quiet  rank_bm25 > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "h7HV_yPpmKdX"
   },
   "outputs": [],
   "source": [
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader('KakaoTalk_group.txt')\n",
    "data = loader.load()\n",
    "\n",
    "print(type(data))\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'KakaoTalk_group.txt'}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader('KakaoTalk_group.txt')\n",
    "data = loader.load()\n",
    "\n",
    "print(type(data))\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "OYhREH6zmMJp"
   },
   "outputs": [],
   "source": [
    "model_name = \"jhgan/ko-sbert-nli\"\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "ko_embedding = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "15n17ed7meJI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<langchain_community.document_loaders.text.TextLoader object at 0x7f2a717d6920>]\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loaders = [\n",
    "    # PyPDFLoader(\"./first.pdf\"),\n",
    "    # PyPDFLoader(\"./fsecond.pdf\"),\n",
    "    TextLoader('KakaoTalk_group.txt')\n",
    "]\n",
    "print(loaders)\n",
    "docs = []\n",
    "for loader in loaders:\n",
    "    docs.extend(loader.load_and_split())\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=50)\n",
    "texts = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "ck2cUaLX4pa3"
   },
   "outputs": [],
   "source": [
    "# initialize the bm25 retriever and faiss retriever\n",
    "bm25_retriever = BM25Retriever.from_documents(texts)\n",
    "bm25_retriever.k = 2\n",
    "\n",
    "\n",
    "\n",
    "embedding = ko_embedding\n",
    "faiss_vectorstore = FAISS.from_documents(texts, ko_embedding)\n",
    "faiss_retriever = faiss_vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "# initialize the ensemble retriever\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[bm25_retriever, faiss_retriever], weights=[0.5, 0.5]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "JcBSDvy3mmlx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'KakaoTalk_group.txt'}\n",
      ":\n",
      "[ê¹€ë¬¸í¬] [ì˜¤í›„ 11:23] 204ì¡°ëŠ” ì§„ì§œ ì•„ë¬´ë„ ëª¨ë¥´ê²ŸìŒ\n",
      "--------------- 2024ë…„ 7ì›” 29ì¼ ì›”ìš”ì¼ ---------------\n",
      "[ì§„ê¸°] [ì˜¤ì „ 12:34] ã…‹ã…‹ã…‹ã…‹ã…‹\n",
      "[ì§„ê¸°] [ì˜¤ì „ 12:35] ë°©ê¸ˆê¹Œì§€ ë„ì»¤ë¥¼ í–ˆì§€ë§Œ ì™„ë£Œí•˜ì§€ ëª»í•´ ë‚´ì¼ê¹Œì§€ ì™„ë£Œí•˜ê² ìŠµë‹ˆë‹¤...;\n",
      "[ì§„ê¸°] [ì˜¤ì „ 12:35] 204ì¡°ëŠ” í•œ ë²ˆ ì°¾ì•„ë³¼ê¹Œìš”?\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'source': 'KakaoTalk_group.txt'}\n",
      ":\n",
      "[ê¹€ë¬¸í¬] [ì˜¤ì „ 8:51] https://youtu.be/T59SXQlneLY?si=bdY01AHKYja83Z6v\n",
      "[ì§„ê¸°] [ì˜¤í›„ 12:58] ì˜ˆë³¸ì•„ ì¹´í˜ì•¼\n",
      "[ë°•ì˜ˆë³¸] [ì˜¤í›„ 12:59] ã…‡ã…‹ã…‡ã…‹\n",
      "[ì§„í˜„ì§€] [ì˜¤í›„ 4:01] íŒŒì¼: ì•„ì´ë””ì–´ íšŒì˜.pdf\n",
      "--------------- 2024ë…„ 7ì›” 14ì¼ ì¼ìš”ì¼ ---------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'source': 'KakaoTalk_group.txt'}\n",
      ":\n",
      "--------------- 2024ë…„ 7ì›” 10ì¼ ìˆ˜ìš”ì¼ ---------------\n",
      "[ê¹€ë¬¸í¬] [ì˜¤ì „ 7:02] @all\n",
      "## ì…”í‹€ë²„ìŠ¤ì•ˆë‚´\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'source': 'KakaoTalk_group.txt'}\n",
      ":\n",
      "1ë°˜ ì—ì´ìŠ¤ ë‹˜ê³¼ ì¹´ì¹´ì˜¤í†¡ ëŒ€í™”\n",
      "ì €ì¥í•œ ë‚ ì§œ : 2024-07-29 13:47:37\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# docs = ensemble_retriever.invoke(\"í˜ì‹ ì •ì±…ê¸ˆìœµê³¼ ê·¹ì €ì‹ ìš©ëŒ€ì¶œëª¨í˜•ì˜ ì°¨ì´\")\n",
    "docs = ensemble_retriever.invoke(\"7ì›” 29ì¼ ëŒ€í™”ë‚´ìš©\")\n",
    "for i in docs:\n",
    "\n",
    "  print(i.metadata)\n",
    "  print(\":\")\n",
    "  print(i.page_content)\n",
    "  print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "c0Jhox_l6qe2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'KakaoTalk_group.txt'}\n",
      ":\n",
      "[ê¹€ë¬¸í¬] [ì˜¤ì „ 8:51] https://youtu.be/T59SXQlneLY?si=bdY01AHKYja83Z6v\n",
      "[ì§„ê¸°] [ì˜¤í›„ 12:58] ì˜ˆë³¸ì•„ ì¹´í˜ì•¼\n",
      "[ë°•ì˜ˆë³¸] [ì˜¤í›„ 12:59] ã…‡ã…‹ã…‡ã…‹\n",
      "[ì§„í˜„ì§€] [ì˜¤í›„ 4:01] íŒŒì¼: ì•„ì´ë””ì–´ íšŒì˜.pdf\n",
      "--------------- 2024ë…„ 7ì›” 14ì¼ ì¼ìš”ì¼ ---------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'source': 'KakaoTalk_group.txt'}\n",
      ":\n",
      "1ë°˜ ì—ì´ìŠ¤ ë‹˜ê³¼ ì¹´ì¹´ì˜¤í†¡ ëŒ€í™”\n",
      "ì €ì¥í•œ ë‚ ì§œ : 2024-07-29 13:47:37\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'source': 'KakaoTalk_group.txt'}\n",
      ":\n",
      "[ë°•ì˜ˆë³¸] [ì˜¤í›„ 2:28] íŒŒì¼: ion.pptx\n",
      "[ê¹€ë¬¸í¬] [ì˜¤í›„ 5:22] https://padlet.com/dudgml1531/ssafy11-4f-s9vaadxudtxeusc4\n",
      "--------------- 2024ë…„ 7ì›” 5ì¼ ê¸ˆìš”ì¼ ---------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'source': 'KakaoTalk_group.txt'}\n",
      ":\n",
      "--------------- 2024ë…„ 7ì›” 25ì¼ ëª©ìš”ì¼ ---------------\n",
      "[ê¹€ë¬¸í¬] [ì˜¤ì „ 8:47] https://youtu.be/dBDkYofMUs4?si=mmDs4MSRbBSvEkMJ\n",
      "[ì§„ê¸°] [ì˜¤ì „ 8:56] https://youtu.be/-m_Kow1fDMo\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "faiss_vectorstore = FAISS.from_documents(texts, ko_embedding)\n",
    "faiss_retriever = faiss_vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "docs = faiss_retriever.invoke(\"7ì›” 29ì¼ ëŒ€í™”ë‚´ìš©\")\n",
    "for i in docs:\n",
    "\n",
    "  print(i.metadata)\n",
    "  print(\":\")\n",
    "  print(i.page_content)\n",
    "  print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "28vloMiumuY_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7ì›” 29ì¼ì— ê¹€ë¬¸í¬ì™€ ì§„ê¸°ê°€ ëŒ€í™”í•œ ë‚´ìš©ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n",
      "\n",
      "[ê¹€ë¬¸í¬] [ì˜¤í›„ 11:23] 204ì¡°ëŠ” ì§„ì§œ ì•„ë¬´ë„ ëª¨ë¥´ê²ŸìŒ\n",
      "[ì§„ê¸°] [ì˜¤í›„ 11:26] ì´ëª¨í‹°ì½˜\n",
      "[ì§„ê¸°] [ì˜¤í›„ 11:36] ë¬¸í¬ë‹˜ì´ ê°€ìë©´ ê°€ì•¼ì¡°.. ì €í¬ê°€ ë¬´ìŠ¨ ì„ íƒê¶Œì´ ã…œ\n",
      "[ê¹€ë¬¸í¬] [ì˜¤í›„ 11:36] ë„¤ ?\n",
      "[ê¹€ë¬¸í¬] [ì˜¤í›„ 11:36] ì´ëª¨í‹°ì½˜\n",
      "\n",
      "ì´ ë‚ ì§œì— ëŒ€í™”í•œ ë‚´ìš©ì€ 204ì¡°ì— ëŒ€í•œ ì–¸ê¸‰ê³¼ í•¨ê»˜ ì§„ê¸°ì™€ ê¹€ë¬¸í¬ê°€ ëŒ€í™”ë¥¼ ì£¼ê³ ë°›ì•˜ìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = 'sk-proj-5g3W1SVHGnmBB7Y2HEMCT3BlbkFJUSvDENpVaFEdRNwZHWlH'\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "openai = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature = 0)\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm = openai,\n",
    "                                 chain_type = \"stuff\",\n",
    "                                 retriever = ensemble_retriever,\n",
    "                                 return_source_documents = True)\n",
    "\n",
    "query = \"7ì›” 29ì¼ ì¹´ì¹´ì˜¤í†¡ ë‚´ìš©\"\n",
    "result = qa(query)\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "LxPfaF0L0c2I"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'KakaoTalk_group.txt'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "1ë°˜ ì—ì´ìŠ¤ ë‹˜ê³¼ ì¹´ì¹´ì˜¤í†¡ ëŒ€í™”\n",
      "ì €ì¥í•œ ë‚ ì§œ : 2024-07-29 13:47:37\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'source': 'KakaoTalk_group.txt'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[ê¹€ë¬¸í¬] [ì˜¤í›„ 11:23] 204ì¡°ëŠ” ì§„ì§œ ì•„ë¬´ë„ ëª¨ë¥´ê²ŸìŒ\n",
      "--------------- 2024ë…„ 7ì›” 29ì¼ ì›”ìš”ì¼ ---------------\n",
      "[ì§„ê¸°] [ì˜¤ì „ 12:34] ã…‹ã…‹ã…‹ã…‹ã…‹\n",
      "[ì§„ê¸°] [ì˜¤ì „ 12:35] ë°©ê¸ˆê¹Œì§€ ë„ì»¤ë¥¼ í–ˆì§€ë§Œ ì™„ë£Œí•˜ì§€ ëª»í•´ ë‚´ì¼ê¹Œì§€ ì™„ë£Œí•˜ê² ìŠµë‹ˆë‹¤...;\n",
      "[ì§„ê¸°] [ì˜¤ì „ 12:35] 204ì¡°ëŠ” í•œ ë²ˆ ì°¾ì•„ë³¼ê¹Œìš”?\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'source': 'KakaoTalk_group.txt'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[ê¹€ë¬¸í¬] [ì˜¤í›„ 11:26] ì´ëª¨í‹°ì½˜\n",
      "[ì§„ê¸°] [ì˜¤í›„ 11:36] ë¬¸í¬ë‹˜ì´ ê°€ìë©´ ê°€ì•¼ì¡°.. ì €í¬ê°€ ë¬´ìŠ¨ ì„ íƒê¶Œì´ ã…œ\n",
      "[ê¹€ë¬¸í¬] [ì˜¤í›„ 11:36] ë„¤ ?\n",
      "[ê¹€ë¬¸í¬] [ì˜¤í›„ 11:36] ì´ëª¨í‹°ì½˜\n",
      "--------------- 2024ë…„ 7ì›” 8ì¼ ì›”ìš”ì¼ ---------------\n",
      "[ì§„í˜„ì§€] [ì˜¤ì „ 12:07] ì €ë„ ì™„ì „ ì¢‹ì•„ìš”~!~!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in result['source_documents']:\n",
    "  print(i.metadata)\n",
    "  print(\"-\"*100)\n",
    "  print(i.page_content)\n",
    "  print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "bxftciR52lYW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì£„ì†¡í•©ë‹ˆë‹¤, 2024-07-28ì— [ë°•ìƒí•„]ì´ ë³´ë‚¸ ë©”ì‹œì§€ì— ëŒ€í•œ ì •ë³´ëŠ” ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í•´ë‹¹ ëŒ€í™” ë‚´ìš©ì´ ì—†ëŠ” ê²ƒ ê°™ìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "faiss_vectorstore = FAISS.from_documents(docs, ko_embedding)\n",
    "faiss_retriever = faiss_vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm = openai,\n",
    "                                 chain_type = \"stuff\",\n",
    "                                 retriever = faiss_retriever,\n",
    "                                 return_source_documents = True)\n",
    "\n",
    "query = \"2024-07-28 ì— [ë°•ìƒí•„]ì´ ë³´ë‚¸ ë©”ì„¸ì§€\"\n",
    "result = qa(query)\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "kId5kM9H3LxY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'KakaoTalk_group.txt'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[ë°•ì˜ˆë³¸] [ì˜¤í›„ 2:28] íŒŒì¼: ion.pptx\n",
      "[ê¹€ë¬¸í¬] [ì˜¤í›„ 5:22] https://padlet.com/dudgml1531/ssafy11-4f-s9vaadxudtxeusc4\n",
      "--------------- 2024ë…„ 7ì›” 5ì¼ ê¸ˆìš”ì¼ ---------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'source': 'KakaoTalk_group.txt'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "--------------- 2024ë…„ 7ì›” 25ì¼ ëª©ìš”ì¼ ---------------\n",
      "[ê¹€ë¬¸í¬] [ì˜¤ì „ 8:47] https://youtu.be/dBDkYofMUs4?si=mmDs4MSRbBSvEkMJ\n",
      "[ì§„ê¸°] [ì˜¤ì „ 8:56] https://youtu.be/-m_Kow1fDMo\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'source': 'KakaoTalk_group.txt'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[ê¹€ë¬¸í¬] [ì˜¤ì „ 8:51] https://youtu.be/T59SXQlneLY?si=bdY01AHKYja83Z6v\n",
      "[ì§„ê¸°] [ì˜¤í›„ 12:58] ì˜ˆë³¸ì•„ ì¹´í˜ì•¼\n",
      "[ë°•ì˜ˆë³¸] [ì˜¤í›„ 12:59] ã…‡ã…‹ã…‡ã…‹\n",
      "[ì§„í˜„ì§€] [ì˜¤í›„ 4:01] íŒŒì¼: ì•„ì´ë””ì–´ íšŒì˜.pdf\n",
      "--------------- 2024ë…„ 7ì›” 14ì¼ ì¼ìš”ì¼ ---------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'source': 'KakaoTalk_group.txt'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "1ë°˜ ì—ì´ìŠ¤ ë‹˜ê³¼ ì¹´ì¹´ì˜¤í†¡ ëŒ€í™”\n",
      "ì €ì¥í•œ ë‚ ì§œ : 2024-07-29 13:47:37\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in result['source_documents']:\n",
    "  print(i.metadata)\n",
    "  print(\"-\"*100)\n",
    "  print(i.page_content)\n",
    "  print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MXZ7dGLm3e8A"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-YinIRx6_--y"
   },
   "source": [
    "## Long Context Reorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "53bnfKrCAPxj"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain, StuffDocumentsChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.document_transformers import (\n",
    "    LongContextReorder,\n",
    ")\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "texts = [\n",
    "    \"ë°”ìŠ¤ì¼“ë³¼ì€ í›Œë¥­í•œ ìŠ¤í¬ì¸ ì…ë‹ˆë‹¤.\",\n",
    "    \"í”Œë¼ì´ ë¯¸ íˆ¬ ë” ë¬¸ì€ ì œê°€ ê°€ì¥ ì¢‹ì•„í•˜ëŠ” ë…¸ë˜ ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤.\",\n",
    "    \"ì…€í‹±ìŠ¤ëŠ” ì œê°€ ê°€ì¥ ì¢‹ì•„í•˜ëŠ” íŒ€ì…ë‹ˆë‹¤.\",\n",
    "    \"ë³´ìŠ¤í„´ ì…€í‹±ìŠ¤ì— ê´€í•œ ë¬¸ì„œì…ë‹ˆë‹¤.\", \"ë³´ìŠ¤í„´ ì…€í‹±ìŠ¤ëŠ” ì œê°€ ê°€ì¥ ì¢‹ì•„í•˜ëŠ” íŒ€ì…ë‹ˆë‹¤.\",\n",
    "    \"ì €ëŠ” ì˜í™” ë³´ëŸ¬ ê°€ëŠ” ê²ƒì„ ì¢‹ì•„í•´ìš”\",\n",
    "    \"ë³´ìŠ¤í„´ ì…€í‹±ìŠ¤ê°€ 20ì ì°¨ë¡œ ì´ê²¼ì–´ìš”\",\n",
    "    \"ì´ê²ƒì€ ê·¸ëƒ¥ ì„ì˜ì˜ í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤.\",\n",
    "    \"ì—˜ë“  ë§ì€ ì§€ë‚œ 15 ë…„ ë™ì•ˆ ìµœê³ ì˜ ê²Œì„ ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤.\",\n",
    "    \"L. ì½”ë„·ì€ ìµœê³ ì˜ ì…€í‹±ìŠ¤ ì„ ìˆ˜ ì¤‘ í•œ ëª…ì…ë‹ˆë‹¤.\",\n",
    "    \"ë˜ë¦¬ ë²„ë“œëŠ” ìƒì§•ì  ì¸ NBA ì„ ìˆ˜ì˜€ìŠµë‹ˆë‹¤.\",\n",
    "]\n",
    "\n",
    "# Create a retriever\n",
    "retriever = Chroma.from_texts(texts, embedding=ko_embedding).as_retriever(\n",
    "    search_kwargs={\"k\": 10}\n",
    ")\n",
    "query = \"ì…€í‹±ìŠ¤ì— ëŒ€í•´ ì–´ë–¤ ì´ì•¼ê¸°ë¥¼ ë“¤ë ¤ì£¼ì‹œê² ì–´ìš”?\"\n",
    "\n",
    "# Get relevant documents ordered by relevance score\n",
    "docs = retriever.get_relevant_documents(query)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x_9sEUx0AX9h"
   },
   "outputs": [],
   "source": [
    "reordering = LongContextReorder()\n",
    "reordered_docs = reordering.transform_documents(docs)\n",
    "\n",
    "# Confirm that the 4 relevant documents are at beginning and end.\n",
    "reordered_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jxdYbGqgA4Sf"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain, StuffDocumentsChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = 'YOUR_API_KEY'\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "document_prompt = PromptTemplate(\n",
    "    input_variables=[\"page_content\"], template=\"{page_content}\"\n",
    ")\n",
    "\n",
    "template = \"\"\"Given this text extracts:\n",
    "-----\n",
    "{context}\n",
    "-----\n",
    "Please answer the following question:\n",
    "{query}\"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    template=template, input_variables=[\"context\", \"query\"]\n",
    ")\n",
    "openai = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature = 0)\n",
    "\n",
    "llm_chain = LLMChain(llm=openai, prompt=prompt)\n",
    "chain = StuffDocumentsChain(\n",
    "    llm_chain=llm_chain,\n",
    "    document_prompt=document_prompt,\n",
    "    document_variable_name=\"context\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cn0oLiy3D_WR"
   },
   "outputs": [],
   "source": [
    "reordered_result = chain.run(input_documents=reordered_docs, query=query)\n",
    "result = chain.run(input_documents=docs, query=query)\n",
    "\n",
    "print(reordered_result)\n",
    "print(\"-\"*100)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JaIjA6LWEdP2"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "_HBhuL23S_YR",
    "digNJMyRtM3k",
    "AK752NU_tHjb",
    "FoTL9eBctWva",
    "zQP9iztA8ApB"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "rag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
