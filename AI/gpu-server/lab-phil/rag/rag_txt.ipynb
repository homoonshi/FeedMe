{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "GiQs8-_Nn_Ml"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q langchain pypdf sentence-transformers chromadb openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VgYrF6FbNkJL",
    "outputId": "825dd743-7700-45c3-dee2-5f6e79ada8b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.10\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_HBhuL23S_YR"
   },
   "source": [
    "## Multi-Query Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Iterator\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.document_loaders.helpers import detect_file_encodings\n",
    "import pandas as pd\n",
    "from langchain_community.document_loaders import CSVLoader\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "class KaKaoTalkLoader(CSVLoader):\n",
    "    def __init__(self, file_path: str, file_suffix:str, encoding: str = \"utf8\", **kwargs):\n",
    "        super().__init__(file_path, encoding=encoding, **kwargs)\n",
    "        # NOTE - choh(2024.04.05) - 파일 확장자 변수 추가\n",
    "        self.file_suffix = file_suffix\n",
    "    \n",
    "    def anonymize_user_id(self, user_id, num_chars_to_anonymize=3):\n",
    "        \"\"\"\n",
    "        비식별화 함수는 주어진 사용자 ID의 앞부분을 '*'로 대체하여 비식별화합니다.\n",
    "\n",
    "        :param user_id: 비식별화할 사용자 ID\n",
    "        :param num_chars_to_anonymize: 비식별화할 문자 수\n",
    "        :return: 비식별화된 사용자 ID\n",
    "        \"\"\"\n",
    "        # 비식별화할 문자 수가 사용자 ID의 길이보다 길 경우, 전체 ID를 '*'로 대체\n",
    "        if num_chars_to_anonymize >= len(user_id):\n",
    "            num_chars_to_anonymize = len(user_id) - 1\n",
    "            return \"*\" * num_chars_to_anonymize\n",
    "\n",
    "        # 앞부분을 '*'로 대체하고 나머지 부분을 원본 ID에서 가져옴\n",
    "        anonymized_id = \"*\" * num_chars_to_anonymize + user_id[num_chars_to_anonymize:]\n",
    "\n",
    "        return anonymized_id\n",
    "    \n",
    "    # NOTE - choh(2024.04.05) - 12시간제를 24시간제로 변환\n",
    "    def process_time_to_24hr_format(self, date_obj, time_str):\n",
    "        \"\"\"\n",
    "        대화 내용중에 시간 표시가 '오전 12:23', '오후 11:23'과 같이 12시간제로 되어 있는 경우, \n",
    "        이를 24시간제로 변환합니다.\n",
    "        \n",
    "        :param date_obj: 대화 내용의 날짜 정보가 담긴 datetime 객체\n",
    "        :praam time_str: 대화 내용의 시간 정보가 담긴 문자열\n",
    "        :return: 24시간제로 변환된 datetime 객체\n",
    "        \"\"\"\n",
    "        \n",
    "        # '오전/오후' 부분과 시간 부분을 분리합니다.\n",
    "        period, time_part = time_str.split(' ', 1)\n",
    "        \n",
    "        # 시간 부분을 시와 분으로 다시 분리합니다.\n",
    "        hour, minute = map(int, time_part.split(':'))\n",
    "        \n",
    "        # '오후'인 경우 12를 더하되, '오후 12시'는 제외합니다.\n",
    "        if period == '오후' and hour != 12:\n",
    "            hour += 12\n",
    "        # '오전 12시'는 0시로 처리합니다.\n",
    "        elif period == '오전' and hour == 12:\n",
    "            hour = 0\n",
    "        \n",
    "        # date_obj과 결합하여 최종 datetime 객체를 생성합니다.\n",
    "        # 여기서 datetime 함수는 위에서 임포트한 datetime 클래스를 사용합니다.\n",
    "        combined_datetime = datetime(date_obj.year, date_obj.month, date_obj.day, hour, minute)\n",
    "        \n",
    "        # pandas의 to_datetime 함수를 사용하여 pandas.Timestamp 객체로 변환합니다.\n",
    "        return pd.to_datetime(combined_datetime)\n",
    "    \n",
    "    # NOTE - choh(2024.04.05) - 대화목록의 날짜 변환 부분을 파싱\n",
    "    def process_date(self, line: str) -> tuple:\n",
    "        \"\"\"\n",
    "        -------- 2024년 4월 5일 화요일 -------- 형태의 날짜를 파싱하고,\n",
    "        파싱 성공 여부와 함께 파싱된 날짜 또는 원래 문자열을 반환합니다.\n",
    "        \n",
    "        :param line: 날짜 문자열\n",
    "        :return: (파싱 성공 여부, 파싱된 날짜 또는 원래 문자열)\n",
    "        \"\"\"\n",
    "        # -------- 2024년 4월 5일 화요일 -------- 날짜가 이상태임\n",
    "        date_match = re.match(r'[-]+ (\\d+년 \\d+월 \\d+일) [^\\d]+', line)\n",
    "        if date_match:\n",
    "            # 2024년 4월 5일, 형태의 날짜 추출\n",
    "            date_pattern = re.compile(r'(\\d+)년 (\\d+)월 (\\d+)일')\n",
    "            match = date_pattern.search(date_match.group(1))\n",
    "            if match:\n",
    "                year, month, day = map(int, match.groups())\n",
    "                return (True, pd.to_datetime(f\"{year}-{month}-{day}\"))\n",
    "        return (False, line)\n",
    "\n",
    "    # NOTE - choh(2024.04.05) - __read_file을 테스트 하기 위한 wrapper 함수\n",
    "    def _read_file_test(self, csvfile) -> Iterator[Document]:\n",
    "        \"\"\"테스트를 위한 래퍼 함수\"\"\"\n",
    "        return self.__read_file(csvfile)\n",
    "    \n",
    "    def __read_file(self, csvfile) -> Iterator[Document]:\n",
    "        # NOTE - choh(2024.04.05) - TXT 형태의 대화 메세지 사전 처리\n",
    "        if self.file_suffix == \".txt\":\n",
    "            \n",
    "            # 전날 날짜 변수 초기화\n",
    "            temp_date = None\n",
    "            i = 0 # 행 번호\n",
    "            for line in csvfile:\n",
    "                \n",
    "                # 이번 줄이 날짜가 맞으면 is_parsed=True, result는 날짜\n",
    "                is_parsed, result = self.process_date(line)\n",
    "                \n",
    "                # 파싱한 문자열이 날짜 패턴에 맞으면, 날짜를 저장\n",
    "                if is_parsed:\n",
    "                    temp_date = result\n",
    "                \n",
    "                # 날짜가 아니면, 체팅이기 때문에, 체팅을 패턴 매칭\n",
    "                else:\n",
    "                    # 초기값 설정\n",
    "                    user = None\n",
    "                    time_12hr = None\n",
    "                    message = None\n",
    "\n",
    "                    # 대화 패턴 찾기\n",
    "                    conversation_match = re.match(r'\\[([^\\]]+)\\] \\[([^\\]]+)\\] (.+)', line)\n",
    "                    if conversation_match:\n",
    "                        user_real = conversation_match.group(1)\n",
    "                        time_12hr = conversation_match.group(2)\n",
    "                        message = conversation_match.group(3).strip()\n",
    "                        \n",
    "                        # 시간을 24시간제로 변환                        \n",
    "                        date = self.process_time_to_24hr_format(temp_date, time_12hr)\n",
    "                        # 사용자 ID 비식별화\n",
    "                        user = self.anonymize_user_id(user_real)\n",
    "                        \n",
    "                        content = f'\"User: {user}, Message: {message}'\n",
    "                        \n",
    "                        metadata = {\n",
    "                            \"date\":  date.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                            \"year\": date.year,\n",
    "                            \"month\": date.month,\n",
    "                            \"day\": date.day,\n",
    "                            \"user\": user,\n",
    "                            \"row\": i,\n",
    "                            \"source\": str(self.file_path),\n",
    "                        }\n",
    "                        i += 1 # 행 번호 증가\n",
    "                        yield Document(page_content=content, metadata=metadata)\n",
    "       \n",
    "        \n",
    "        # NOTE - choh(2024.04.05) - 기존 코드, csv 파일인 경우\n",
    "        else:\n",
    "            df = pd.read_csv(csvfile)\n",
    "            df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "            df[\"Date_strf\"] = df[\"Date\"].dt.strftime(\"%Y-%m-%d %H:%M:%S\").astype(str)\n",
    "            for i, row in df.iterrows():\n",
    "                date = row[\"Date\"]\n",
    "                user = self.anonymize_user_id(row[\"User\"])\n",
    "                content = f'\"User: {user}, Message: {row[\"Message\"]}'\n",
    "\n",
    "                metadata = {\n",
    "                    \"date\": row[\"Date_strf\"],\n",
    "                    \"year\": date.year,\n",
    "                    \"month\": date.month,\n",
    "                    \"day\": date.day,\n",
    "                    \"user\": user,\n",
    "                    \"row\": i,\n",
    "                    \"source\": str(self.file_path),\n",
    "                }\n",
    "                yield Document(page_content=content, metadata=metadata)\n",
    "\n",
    "    def lazy_load(self) -> Iterator[Document]:\n",
    "        try:\n",
    "            with open(self.file_path, newline=\"\", encoding=self.encoding) as csvfile:\n",
    "                yield from self.__read_file(csvfile)\n",
    "      \n",
    "        except UnicodeDecodeError as e:\n",
    "            if self.autodetect_encoding:\n",
    "                detected_encodings = detect_file_encodings(self.file_path)\n",
    "                for encoding in detected_encodings:\n",
    "                    try:\n",
    "                        with open(\n",
    "                            self.file_path, newline=\"\", encoding=encoding.encoding\n",
    "                        ) as csvfile:\n",
    "                            yield from self.__read_file(csvfile)\n",
    "                            break\n",
    "                    except UnicodeDecodeError:\n",
    "                        continue\n",
    "            else:\n",
    "                raise RuntimeError(f\"Error loading {self.file_path}\") from e\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error loading {self.file_path}\") from e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Y3TWw12cSqRw"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n",
      "/home/j-i11b104/.local/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n",
      "/home/j-i11b104/.local/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "# Build a sample vectorDB\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "import kakaotalk_loader as kakao\n",
    "\n",
    "file_path = \"KakaoTalk_group.txt\"\n",
    "file_suffix = \".txt\"  # Change to \".csv\" if you're using a CSV file\n",
    "loader = kakao.KaKaoTalkLoader(file_path, file_suffix, encoding=\"utf8\")\n",
    "\n",
    "# Load blog post\n",
    "loader = WebBaseLoader(\"https://corin-e.tistory.com/entry/1-%EC%82%BC%EC%84%B1-%EC%B2%AD%EB%85%84-SW-%EC%95%84%EC%B9%B4%EB%8D%B0%EB%AF%B8%EC%8B%B8%ED%94%BC-SSAFY-%EC%82%BC%EC%88%98-%ED%9B%84%EA%B8%B0\")\n",
    "data = loader.load()\n",
    "\n",
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=30)\n",
    "splits = text_splitter.split_documents(data)\n",
    "\n",
    "# VectorDB\n",
    "model_name = \"jhgan/ko-sbert-nli\"\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "ko_embedding = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "# sk-proj-5g3W1SVHGnmBB7Y2HEMCT3BlbkFJUSvDENpVaFEdRNwZHWlH\n",
    "# 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\n",
    "open_key = \"sk-proj-5g3W1SVHGnmBB7Y2HEMCT3BlbkFJUSvDENpVaFEdRNwZHWlH\"\n",
    "vectordb = Chroma.from_documents(documents=splits, embedding=ko_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "epoTy2FNSuqz"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/j-i11b104/.local/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "question = \"작성자의 생각\"\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, openai_api_key = open_key)\n",
    "retriever_from_llm = MultiQueryRetriever.from_llm(\n",
    "    retriever=vectordb.as_retriever(), llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "eU6X_HctSumR"
   },
   "outputs": [],
   "source": [
    "# Set logging for the queries\n",
    "import logging\n",
    "\n",
    "logging.basicConfig()\n",
    "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "pXYmox2MSuZ-"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/j-i11b104/.local/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "INFO:langchain.retrievers.multi_query:Generated queries: ['1. What are the thoughts of the author?', \"2. What is the author's perspective?\", \"3. Can you provide insights into the author's mindset?\"]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_docs = retriever_from_llm.get_relevant_documents(query=question)\n",
    "len(unique_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "MIz2l4OaS75w"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'description': '가드니의 블로그', 'language': 'ko', 'source': 'https://corin-e.tistory.com/entry/1-%EC%82%BC%EC%84%B1-%EC%B2%AD%EB%85%84-SW-%EC%95%84%EC%B9%B4%EB%8D%B0%EB%AF%B8%EC%8B%B8%ED%94%BC-SSAFY-%EC%82%BC%EC%88%98-%ED%9B%84%EA%B8%B0', 'title': '1) 삼성 청년 SW 아카데미(싸피, SSAFY) 삼수 후기'}, page_content='안녕하세요! 정보 공유 해주셔서 감사합니다.\\n혹시 면접은 주말에 보셨을까요?ㅜㅜ\\n밍밍수\\n\\n\\n\\n\\n인덱스 : 0  1  2  3  4  5  6  7  8  9 10\\n방  향 :    왼 ⋯\\n코딩질문자\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCopyright © Kakao Corp. All rights reserved.\\n\\n\\n\\n관련사이트\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n티스토리툴바\\n🪨 🪨 🪨구독하기'),\n",
       " Document(metadata={'description': '가드니의 블로그', 'language': 'ko', 'source': 'https://corin-e.tistory.com/entry/1-%EC%82%BC%EC%84%B1-%EC%B2%AD%EB%85%84-SW-%EC%95%84%EC%B9%B4%EB%8D%B0%EB%AF%B8%EC%8B%B8%ED%94%BC-SSAFY-%EC%82%BC%EC%88%98-%ED%9B%84%EA%B8%B0', 'title': '1) 삼성 청년 SW 아카데미(싸피, SSAFY) 삼수 후기'}, page_content='각종 후기 (2) \\n\\n\\n\\n\\n\\n\\n\\n최근글과 인기글\\n\\n최근글\\n인기글\\n\\n\\n\\n\\n\\n\\n스프링 + MongoDB) @JsonProperty @JsonNaming 인식 안됨\\n2023.07.13 22:01\\n\\n\\n\\n\\n\\n\\n\\nERESOLVE unable to resolve dependency tree - npm install 오류\\n2023.03.11 23:48\\n\\n\\n\\n\\n\\n\\n프로그래머스 스프링 코딩(2023년 2월) 후기\\n2023.02.11 12:18\\n\\n\\n\\n\\n\\n\\n\\n\\n신입 IT 개발자 면접 질문 총 정리 - 인성/회사/직무/경험/기술\\n2022.12.12 20:32\\n\\n\\n\\n\\n\\n\\n\\n신입 IT 개발자 코딩 테스트 후기 - 언어, 공부 방법, 난이도 등\\n2022.12.21 01:37\\n\\n\\n\\n\\n\\n\\n\\n1) 삼성 청년 SW 아카데미(싸피, SSAFY) 삼수 후기\\n2022.12.21 20:30\\n\\n\\n\\n\\n\\n\\n\\n최근댓글\\n\\n\\n\\n정말 도움이 많이 되었습니다. 복 받으세요 ~!  감사합니다. :)\\n포로로'),\n",
       " Document(metadata={'description': '가드니의 블로그', 'language': 'ko', 'source': 'https://corin-e.tistory.com/entry/1-%EC%82%BC%EC%84%B1-%EC%B2%AD%EB%85%84-SW-%EC%95%84%EC%B9%B4%EB%8D%B0%EB%AF%B8%EC%8B%B8%ED%94%BC-SSAFY-%EC%82%BC%EC%88%98-%ED%9B%84%EA%B8%B0', 'title': '1) 삼성 청년 SW 아카데미(싸피, SSAFY) 삼수 후기'}, page_content='관련글\\n\\n\\n\\n\\n\\n5-1) 넥토리얼 지원 후기 - 코딩 테스트\\n2022.12.22\\n\\n\\n\\n\\n\\n\\n\\n4) NC소프트 신입 지원 후기 - 코딩 테스트\\n2022.12.22\\n\\n\\n\\n\\n\\n\\n\\n3) 넷마블 컴퍼니 신입 지원 후기 - 코딩 테스트\\n2022.12.22\\n\\n\\n\\n\\n\\n\\n\\n2) 프로그래머스 백엔드 데브매칭/윈터코딩 후기 - 코딩 테스트, 서류 전형\\n2022.12.22\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n댓글 7\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n비밀글\\n\\n등록\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n돌돌돌\\n\\n\\n\\n\\n\\n\\n\\n방문자수Total\\n246,531\\n\\nToday : 89\\nYesterday : 173\\n\\n\\n\\n\\n\\n\\n 분류 전체보기 (55) \\n JAVA (1) \\n JavaScript (1) \\n 백준 (21) \\n 자료구조 (2) \\n 기타 (2) \\n 오류 해결 (4) \\n 개발자 취업 준비 (22) \\n 준비 방법 (2) \\n 취준 후기 (20) \\n\\n\\n 각종 후기 (2)'),\n",
       " Document(metadata={'description': '가드니의 블로그', 'language': 'ko', 'source': 'https://corin-e.tistory.com/entry/1-%EC%82%BC%EC%84%B1-%EC%B2%AD%EB%85%84-SW-%EC%95%84%EC%B9%B4%EB%8D%B0%EB%AF%B8%EC%8B%B8%ED%94%BC-SSAFY-%EC%82%BC%EC%88%98-%ED%9B%84%EA%B8%B0', 'title': '1) 삼성 청년 SW 아카데미(싸피, SSAFY) 삼수 후기'}, page_content=\"공유하기\\n\\n게시글 관리\\n\\n\\n🪨 🪨 🪨\\n\\n\\n저작자표시\\n\\n\\n\\n \\n\\n\\n\\n'개발자 취업 준비 > 취준 후기' 카테고리의 다른 글\\n\\n\\n5-2) 넥토리얼 지원 후기 - 면접\\xa0\\xa0(1)\\n2022.12.22\\n\\n\\n5-1) 넥토리얼 지원 후기 - 코딩 테스트\\xa0\\xa0(1)\\n2022.12.22\\n\\n\\n4) NC소프트 신입 지원 후기 - 코딩 테스트\\xa0\\xa0(2)\\n2022.12.22\\n\\n\\n3) 넷마블 컴퍼니 신입 지원 후기 - 코딩 테스트\\xa0\\xa0(0)\\n2022.12.22\\n\\n\\n2) 프로그래머스 백엔드 데브매칭/윈터코딩 후기 - 코딩 테스트, 서류 전형\\xa0\\xa0(0)\\n2022.12.22\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTag\\nCT, IT, SSAFY, SW 적성 진단, 개발자, 비전공, 삼성, 삼성 청년 SW 아카데미, 싸피\\n\\n\\n\\n'개발자 취업 준비/취준 후기'의 다른글\\n\\n\\n\\n현재글1) 삼성 청년 SW 아카데미(싸피, SSAFY) 삼수 후기\\n\\n다음글2) 프로그래머스 백엔드 데브매칭/윈터코딩 후기 - 코딩 테스트, 서류 전형\\n\\n\\n\\n\\n\\n관련글\")]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "digNJMyRtM3k"
   },
   "source": [
    "## 기본 Parent-document Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "id": "tZOk-3LOnb-0"
   },
   "outputs": [],
   "source": [
    "from langchain.retrievers import ParentDocumentRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "id": "f0ONC3wZn62e"
   },
   "outputs": [],
   "source": [
    "from langchain.storage import InMemoryStore\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "id": "VpJhZWThn7gG"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Maybe you meant '==' or ':=' instead of '='? (2736647683.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[123], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    loader = TextLoader('KakaoTalk_group.txt')\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax. Maybe you meant '==' or ':=' instead of '='?\n"
     ]
    }
   ],
   "source": [
    "loaders = [\n",
    "    loader = TextLoader('KakaoTalk_group.txt')\n",
    "    # PyPDFLoader(\"/content/drive/MyDrive/강의 자료/[복지이슈 FOCUS 15ȣ] 경기도 극저신용대출심사모형 개발을 위한 국내 신용정보 활용가능성 탐색.pdf\"),\n",
    "    # PyPDFLoader(\"/content/drive/MyDrive/강의 자료/[이슈리포트 2022-2호] 혁신성장 정책금융 동향.pdf\"),\n",
    "]\n",
    "docs = []\n",
    "for loader in loaders:\n",
    "    docs.extend(loader.load_and_split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "id": "KBb-1ym5pKRG"
   },
   "outputs": [],
   "source": [
    "model_name = \"jhgan/ko-sbert-nli\"\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "ko_embedding = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "id": "fyixQa5SpHOW"
   },
   "outputs": [],
   "source": [
    "# This text splitter is used to create the child documents\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=200)\n",
    "# The vectorstore to use to index the child chunks\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"full_documents\", embedding_function=ko_embedding\n",
    ")\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryStore()\n",
    "retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "id": "IyVXzfq0pHME"
   },
   "outputs": [],
   "source": [
    "retriever.add_documents(docs, ids=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "id": "L8z2B2rKrAv3"
   },
   "outputs": [],
   "source": [
    "sub_docs = vectorstore.similarity_search(\"7월 29일 대화 내용\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "id": "_axFoMvnrA-h"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "글 길이: 46\n",
      "\n",
      "\n",
      "1반 에이스 님과 카카오톡 대화\n",
      "저장한 날짜 : 2024-07-29 13:47:37\n"
     ]
    }
   ],
   "source": [
    "print(\"글 길이: {}\\n\\n\".format(len(sub_docs[0].page_content)))\n",
    "print(sub_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "id": "W68G7ITorThj"
   },
   "outputs": [],
   "source": [
    "retrieved_docs = retriever.get_relevant_documents(\"7월 29일 대화 내용\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "id": "Uzl334J8rXEQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "글 길이: 46\n",
      "\n",
      "\n",
      "1반 에이스 님과 카카오톡 대화\n",
      "저장한 날짜 : 2024-07-29 13:47:37\n"
     ]
    }
   ],
   "source": [
    "print(\"글 길이: {}\\n\\n\".format(len(retrieved_docs[0].page_content)))\n",
    "print(retrieved_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AK752NU_tHjb"
   },
   "source": [
    "## 본문의 Full_chunk가 너무 길때"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "id": "q64Iw5Wbrd94"
   },
   "outputs": [],
   "source": [
    "# This text splitter is used to create the parent documents\n",
    "parent_splitter = RecursiveCharacterTextSplitter(chunk_size=800)\n",
    "# This text splitter is used to create the child documents\n",
    "# It should create documents smaller than the parent\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=200)\n",
    "# The vectorstore to use to index the child chunks\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"split_parents\", embedding_function=ko_embedding\n",
    ")\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryStore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "id": "1TVbBTmgr_CA"
   },
   "outputs": [],
   "source": [
    "retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    "    parent_splitter=parent_splitter,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "id": "K9bPu58hsAfP"
   },
   "outputs": [],
   "source": [
    "retriever.add_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "id": "KLvoTdeksUne"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(store.yield_keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "id": "K0Te96C7sVxA"
   },
   "outputs": [],
   "source": [
    "sub_docs = vectorstore.similarity_search(\"7월 29일 대화 내용\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "id": "qJ6Qb_E_sZCb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1반 에이스 님과 카카오톡 대화\n",
      "저장한 날짜 : 2024-07-29 13:47:37\n"
     ]
    }
   ],
   "source": [
    "print(sub_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "id": "GXioc_E3so_U"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sub_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "id": "bL6F-aBJsZyv"
   },
   "outputs": [],
   "source": [
    "retrieved_docs = retriever.get_relevant_documents(\"7월 29일 대화 내용\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "id": "EnhD6dY_sfKq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1반 에이스 님과 카카오톡 대화\n",
      "저장한 날짜 : 2024-07-29 13:47:37\n"
     ]
    }
   ],
   "source": [
    "print(retrieved_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "id": "UhxrZ1zAscVP"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(retrieved_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FoTL9eBctWva"
   },
   "source": [
    "## Self-querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "_DmHA3cgtZcJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting lark\n",
      "  Downloading lark-1.1.9-py3-none-any.whl.metadata (1.9 kB)\n",
      "Downloading lark-1.1.9-py3-none-any.whl (111 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.7/111.7 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: lark\n",
      "Successfully installed lark-1.1.9\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install lark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7TJLhLfQthCt"
   },
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "docs = [\n",
    "    Document(\n",
    "        page_content=\"A bunch of scientists bring back dinosaurs and mayhem breaks loose\",\n",
    "        metadata={\"year\": 1993, \"rating\": 7.7, \"genre\": \"science fiction\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Leo DiCaprio gets lost in a dream within a dream within a dream within a ...\",\n",
    "        metadata={\"year\": 2010, \"director\": \"Christopher Nolan\", \"rating\": 8.2},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea\",\n",
    "        metadata={\"year\": 2006, \"director\": \"Satoshi Kon\", \"rating\": 8.6},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"A bunch of normal-sized women are supremely wholesome and some men pine after them\",\n",
    "        metadata={\"year\": 2019, \"director\": \"Greta Gerwig\", \"rating\": 8.3},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Toys come alive and have a blast doing so\",\n",
    "        metadata={\"year\": 1995, \"genre\": \"animated\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Three men walk into the Zone, three men walk out of the Zone\",\n",
    "        metadata={\n",
    "            \"year\": 1979,\n",
    "            \"director\": \"Andrei Tarkovsky\",\n",
    "            \"genre\": \"thriller\",\n",
    "            \"rating\": 9.9,\n",
    "        },\n",
    "    ),\n",
    "]\n",
    "vectorstore = Chroma.from_documents(docs, ko_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9x2PsJ9e7tuH"
   },
   "outputs": [],
   "source": [
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "metadata_field_info = [\n",
    "    AttributeInfo(\n",
    "        name=\"genre\",\n",
    "        description=\"The genre of the movie. One of ['science fiction', 'comedy', 'drama', 'thriller', 'romance', 'action', 'animated']\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"year\",\n",
    "        description=\"The year the movie was released\",\n",
    "        type=\"integer\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"director\",\n",
    "        description=\"The name of the movie director\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"rating\", description=\"A 1-10 rating for the movie\", type=\"float\"\n",
    "    ),\n",
    "]\n",
    "document_content_description = \"Brief summary of a movie\"\n",
    "llm = ChatOpenAI(temperature=0, openai_api_key = \"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\")\n",
    "retriever = SelfQueryRetriever.from_llm(\n",
    "    llm,\n",
    "    vectorstore,\n",
    "    document_content_description,\n",
    "    metadata_field_info,\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "foYY4IEA74Dr"
   },
   "outputs": [],
   "source": [
    "retriever.get_relevant_documents(\"what are some movies rated higher than 8.5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zQP9iztA8ApB"
   },
   "source": [
    "## Time-weighted vector store Retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OKEps8vK8GIm"
   },
   "source": [
    "Scoring 방법 = *semantic_similarity + (1.0 - decay_rate) ^ hours_passed*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "zKL8WJ_P8UEG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q faiss-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "uLG48BJY8AEW"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'faiss'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datetime, timedelta\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfaiss\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocstore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InMemoryDocstore\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mretrievers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TimeWeightedVectorStoreRetriever\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'faiss'"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "import faiss\n",
    "from langchain.docstore import InMemoryDocstore\n",
    "from langchain.retrievers import TimeWeightedVectorStoreRetriever\n",
    "from langchain.schema import Document\n",
    "from langchain_community.vectorstores import FAISS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lv5TtUnK8Mml"
   },
   "outputs": [],
   "source": [
    "# Initialize the vectorstore as empty\n",
    "embedding_size = 768\n",
    "index = faiss.IndexFlatL2(embedding_size)\n",
    "vectorstore = FAISS(ko_embedding, index, InMemoryDocstore({}), {})\n",
    "retriever = TimeWeightedVectorStoreRetriever(\n",
    "    vectorstore=vectorstore, decay_rate=0.99, k=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vL-HP-9L8OIq"
   },
   "outputs": [],
   "source": [
    "yesterday = datetime.now() - timedelta(days=1)\n",
    "retriever.add_documents(\n",
    "    [Document(page_content=\"영어는 훌륭합니다.\", metadata={\"last_accessed_at\": yesterday})]\n",
    ")\n",
    "retriever.add_documents([Document(page_content=\"한국어는 훌륭합니다\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q6KCbXXV_d4F"
   },
   "outputs": [],
   "source": [
    "# \"Hello World\" is returned first because it is most salient, and the decay rate is close to 0., meaning it's still recent enough\n",
    "retriever.get_relevant_documents(\"영어가 좋아요\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sjtkFgOGfemK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ltRw8HCmmECS"
   },
   "source": [
    "## Ensemble Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "QN9Qk5_LkeQH"
   },
   "outputs": [],
   "source": [
    "!pip install -q langchain pypdf sentence-transformers chromadb langchain-openai faiss-gpu --upgrade --quiet  rank_bm25 > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "h7HV_yPpmKdX"
   },
   "outputs": [],
   "source": [
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader('KakaoTalk_group.txt')\n",
    "data = loader.load()\n",
    "\n",
    "print(type(data))\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'KakaoTalk_group.txt'}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader('KakaoTalk_group.txt')\n",
    "data = loader.load()\n",
    "\n",
    "print(type(data))\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "OYhREH6zmMJp"
   },
   "outputs": [],
   "source": [
    "model_name = \"jhgan/ko-sbert-nli\"\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "ko_embedding = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "15n17ed7meJI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<langchain_community.document_loaders.text.TextLoader object at 0x7f2a717d6920>]\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loaders = [\n",
    "    # PyPDFLoader(\"./first.pdf\"),\n",
    "    # PyPDFLoader(\"./fsecond.pdf\"),\n",
    "    TextLoader('KakaoTalk_group.txt')\n",
    "]\n",
    "print(loaders)\n",
    "docs = []\n",
    "for loader in loaders:\n",
    "    docs.extend(loader.load_and_split())\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=50)\n",
    "texts = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "ck2cUaLX4pa3"
   },
   "outputs": [],
   "source": [
    "# initialize the bm25 retriever and faiss retriever\n",
    "bm25_retriever = BM25Retriever.from_documents(texts)\n",
    "bm25_retriever.k = 2\n",
    "\n",
    "\n",
    "\n",
    "embedding = ko_embedding\n",
    "faiss_vectorstore = FAISS.from_documents(texts, ko_embedding)\n",
    "faiss_retriever = faiss_vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "# initialize the ensemble retriever\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[bm25_retriever, faiss_retriever], weights=[0.5, 0.5]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "JcBSDvy3mmlx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'KakaoTalk_group.txt'}\n",
      ":\n",
      "[김문희] [오후 11:23] 204조는 진짜 아무도 모르겟음\n",
      "--------------- 2024년 7월 29일 월요일 ---------------\n",
      "[진기] [오전 12:34] ㅋㅋㅋㅋㅋ\n",
      "[진기] [오전 12:35] 방금까지 도커를 했지만 완료하지 못해 내일까지 완료하겠습니다...;\n",
      "[진기] [오전 12:35] 204조는 한 번 찾아볼까요?\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'source': 'KakaoTalk_group.txt'}\n",
      ":\n",
      "[김문희] [오전 8:51] https://youtu.be/T59SXQlneLY?si=bdY01AHKYja83Z6v\n",
      "[진기] [오후 12:58] 예본아 카페야\n",
      "[박예본] [오후 12:59] ㅇㅋㅇㅋ\n",
      "[진현지] [오후 4:01] 파일: 아이디어 회의.pdf\n",
      "--------------- 2024년 7월 14일 일요일 ---------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'source': 'KakaoTalk_group.txt'}\n",
      ":\n",
      "--------------- 2024년 7월 10일 수요일 ---------------\n",
      "[김문희] [오전 7:02] @all\n",
      "## 셔틀버스안내\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'source': 'KakaoTalk_group.txt'}\n",
      ":\n",
      "1반 에이스 님과 카카오톡 대화\n",
      "저장한 날짜 : 2024-07-29 13:47:37\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# docs = ensemble_retriever.invoke(\"혁신정책금융과 극저신용대출모형의 차이\")\n",
    "docs = ensemble_retriever.invoke(\"7월 29일 대화내용\")\n",
    "for i in docs:\n",
    "\n",
    "  print(i.metadata)\n",
    "  print(\":\")\n",
    "  print(i.page_content)\n",
    "  print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "c0Jhox_l6qe2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'KakaoTalk_group.txt'}\n",
      ":\n",
      "[김문희] [오전 8:51] https://youtu.be/T59SXQlneLY?si=bdY01AHKYja83Z6v\n",
      "[진기] [오후 12:58] 예본아 카페야\n",
      "[박예본] [오후 12:59] ㅇㅋㅇㅋ\n",
      "[진현지] [오후 4:01] 파일: 아이디어 회의.pdf\n",
      "--------------- 2024년 7월 14일 일요일 ---------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'source': 'KakaoTalk_group.txt'}\n",
      ":\n",
      "1반 에이스 님과 카카오톡 대화\n",
      "저장한 날짜 : 2024-07-29 13:47:37\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'source': 'KakaoTalk_group.txt'}\n",
      ":\n",
      "[박예본] [오후 2:28] 파일: ion.pptx\n",
      "[김문희] [오후 5:22] https://padlet.com/dudgml1531/ssafy11-4f-s9vaadxudtxeusc4\n",
      "--------------- 2024년 7월 5일 금요일 ---------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'source': 'KakaoTalk_group.txt'}\n",
      ":\n",
      "--------------- 2024년 7월 25일 목요일 ---------------\n",
      "[김문희] [오전 8:47] https://youtu.be/dBDkYofMUs4?si=mmDs4MSRbBSvEkMJ\n",
      "[진기] [오전 8:56] https://youtu.be/-m_Kow1fDMo\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "faiss_vectorstore = FAISS.from_documents(texts, ko_embedding)\n",
    "faiss_retriever = faiss_vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "docs = faiss_retriever.invoke(\"7월 29일 대화내용\")\n",
    "for i in docs:\n",
    "\n",
    "  print(i.metadata)\n",
    "  print(\":\")\n",
    "  print(i.page_content)\n",
    "  print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "28vloMiumuY_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7월 29일에 김문희와 진기가 대화한 내용은 다음과 같습니다:\n",
      "\n",
      "[김문희] [오후 11:23] 204조는 진짜 아무도 모르겟음\n",
      "[진기] [오후 11:26] 이모티콘\n",
      "[진기] [오후 11:36] 문희님이 가자면 가야조.. 저희가 무슨 선택권이 ㅜ\n",
      "[김문희] [오후 11:36] 네 ?\n",
      "[김문희] [오후 11:36] 이모티콘\n",
      "\n",
      "이 날짜에 대화한 내용은 204조에 대한 언급과 함께 진기와 김문희가 대화를 주고받았습니다.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = 'sk-proj-5g3W1SVHGnmBB7Y2HEMCT3BlbkFJUSvDENpVaFEdRNwZHWlH'\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "openai = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature = 0)\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm = openai,\n",
    "                                 chain_type = \"stuff\",\n",
    "                                 retriever = ensemble_retriever,\n",
    "                                 return_source_documents = True)\n",
    "\n",
    "query = \"7월 29일 카카오톡 내용\"\n",
    "result = qa(query)\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "LxPfaF0L0c2I"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'KakaoTalk_group.txt'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "1반 에이스 님과 카카오톡 대화\n",
      "저장한 날짜 : 2024-07-29 13:47:37\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'source': 'KakaoTalk_group.txt'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[김문희] [오후 11:23] 204조는 진짜 아무도 모르겟음\n",
      "--------------- 2024년 7월 29일 월요일 ---------------\n",
      "[진기] [오전 12:34] ㅋㅋㅋㅋㅋ\n",
      "[진기] [오전 12:35] 방금까지 도커를 했지만 완료하지 못해 내일까지 완료하겠습니다...;\n",
      "[진기] [오전 12:35] 204조는 한 번 찾아볼까요?\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'source': 'KakaoTalk_group.txt'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[김문희] [오후 11:26] 이모티콘\n",
      "[진기] [오후 11:36] 문희님이 가자면 가야조.. 저희가 무슨 선택권이 ㅜ\n",
      "[김문희] [오후 11:36] 네 ?\n",
      "[김문희] [오후 11:36] 이모티콘\n",
      "--------------- 2024년 7월 8일 월요일 ---------------\n",
      "[진현지] [오전 12:07] 저도 완전 좋아요~!~!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in result['source_documents']:\n",
    "  print(i.metadata)\n",
    "  print(\"-\"*100)\n",
    "  print(i.page_content)\n",
    "  print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "bxftciR52lYW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "죄송합니다, 2024-07-28에 [박상필]이 보낸 메시지에 대한 정보는 제공되지 않았습니다. 해당 대화 내용이 없는 것 같습니다.\n"
     ]
    }
   ],
   "source": [
    "faiss_vectorstore = FAISS.from_documents(docs, ko_embedding)\n",
    "faiss_retriever = faiss_vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm = openai,\n",
    "                                 chain_type = \"stuff\",\n",
    "                                 retriever = faiss_retriever,\n",
    "                                 return_source_documents = True)\n",
    "\n",
    "query = \"2024-07-28 에 [박상필]이 보낸 메세지\"\n",
    "result = qa(query)\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "kId5kM9H3LxY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'KakaoTalk_group.txt'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[박예본] [오후 2:28] 파일: ion.pptx\n",
      "[김문희] [오후 5:22] https://padlet.com/dudgml1531/ssafy11-4f-s9vaadxudtxeusc4\n",
      "--------------- 2024년 7월 5일 금요일 ---------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'source': 'KakaoTalk_group.txt'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "--------------- 2024년 7월 25일 목요일 ---------------\n",
      "[김문희] [오전 8:47] https://youtu.be/dBDkYofMUs4?si=mmDs4MSRbBSvEkMJ\n",
      "[진기] [오전 8:56] https://youtu.be/-m_Kow1fDMo\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'source': 'KakaoTalk_group.txt'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[김문희] [오전 8:51] https://youtu.be/T59SXQlneLY?si=bdY01AHKYja83Z6v\n",
      "[진기] [오후 12:58] 예본아 카페야\n",
      "[박예본] [오후 12:59] ㅇㅋㅇㅋ\n",
      "[진현지] [오후 4:01] 파일: 아이디어 회의.pdf\n",
      "--------------- 2024년 7월 14일 일요일 ---------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'source': 'KakaoTalk_group.txt'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "1반 에이스 님과 카카오톡 대화\n",
      "저장한 날짜 : 2024-07-29 13:47:37\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in result['source_documents']:\n",
    "  print(i.metadata)\n",
    "  print(\"-\"*100)\n",
    "  print(i.page_content)\n",
    "  print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MXZ7dGLm3e8A"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-YinIRx6_--y"
   },
   "source": [
    "## Long Context Reorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "53bnfKrCAPxj"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain, StuffDocumentsChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.document_transformers import (\n",
    "    LongContextReorder,\n",
    ")\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "texts = [\n",
    "    \"바스켓볼은 훌륭한 스포츠입니다.\",\n",
    "    \"플라이 미 투 더 문은 제가 가장 좋아하는 노래 중 하나입니다.\",\n",
    "    \"셀틱스는 제가 가장 좋아하는 팀입니다.\",\n",
    "    \"보스턴 셀틱스에 관한 문서입니다.\", \"보스턴 셀틱스는 제가 가장 좋아하는 팀입니다.\",\n",
    "    \"저는 영화 보러 가는 것을 좋아해요\",\n",
    "    \"보스턴 셀틱스가 20점차로 이겼어요\",\n",
    "    \"이것은 그냥 임의의 텍스트입니다.\",\n",
    "    \"엘든 링은 지난 15 년 동안 최고의 게임 중 하나입니다.\",\n",
    "    \"L. 코넷은 최고의 셀틱스 선수 중 한 명입니다.\",\n",
    "    \"래리 버드는 상징적 인 NBA 선수였습니다.\",\n",
    "]\n",
    "\n",
    "# Create a retriever\n",
    "retriever = Chroma.from_texts(texts, embedding=ko_embedding).as_retriever(\n",
    "    search_kwargs={\"k\": 10}\n",
    ")\n",
    "query = \"셀틱스에 대해 어떤 이야기를 들려주시겠어요?\"\n",
    "\n",
    "# Get relevant documents ordered by relevance score\n",
    "docs = retriever.get_relevant_documents(query)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x_9sEUx0AX9h"
   },
   "outputs": [],
   "source": [
    "reordering = LongContextReorder()\n",
    "reordered_docs = reordering.transform_documents(docs)\n",
    "\n",
    "# Confirm that the 4 relevant documents are at beginning and end.\n",
    "reordered_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jxdYbGqgA4Sf"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain, StuffDocumentsChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = 'YOUR_API_KEY'\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "document_prompt = PromptTemplate(\n",
    "    input_variables=[\"page_content\"], template=\"{page_content}\"\n",
    ")\n",
    "\n",
    "template = \"\"\"Given this text extracts:\n",
    "-----\n",
    "{context}\n",
    "-----\n",
    "Please answer the following question:\n",
    "{query}\"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    template=template, input_variables=[\"context\", \"query\"]\n",
    ")\n",
    "openai = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature = 0)\n",
    "\n",
    "llm_chain = LLMChain(llm=openai, prompt=prompt)\n",
    "chain = StuffDocumentsChain(\n",
    "    llm_chain=llm_chain,\n",
    "    document_prompt=document_prompt,\n",
    "    document_variable_name=\"context\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cn0oLiy3D_WR"
   },
   "outputs": [],
   "source": [
    "reordered_result = chain.run(input_documents=reordered_docs, query=query)\n",
    "result = chain.run(input_documents=docs, query=query)\n",
    "\n",
    "print(reordered_result)\n",
    "print(\"-\"*100)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JaIjA6LWEdP2"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "_HBhuL23S_YR",
    "digNJMyRtM3k",
    "AK752NU_tHjb",
    "FoTL9eBctWva",
    "zQP9iztA8ApB"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "rag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
