{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "GiQs8-_Nn_Ml"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q langchain pypdf sentence-transformers chromadb openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VgYrF6FbNkJL",
    "outputId": "825dd743-7700-45c3-dee2-5f6e79ada8b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.10\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_HBhuL23S_YR"
   },
   "source": [
    "## Multi-Query Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Iterator\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.document_loaders.helpers import detect_file_encodings\n",
    "import pandas as pd\n",
    "from langchain_community.document_loaders import CSVLoader\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "class KaKaoTalkLoader(CSVLoader):\n",
    "    def __init__(self, file_path: str, file_suffix:str, encoding: str = \"utf8\", **kwargs):\n",
    "        super().__init__(file_path, encoding=encoding, **kwargs)\n",
    "        # NOTE - choh(2024.04.05) - 파일 확장자 변수 추가\n",
    "        self.file_suffix = file_suffix\n",
    "    \n",
    "    def anonymize_user_id(self, user_id, num_chars_to_anonymize=3):\n",
    "        \"\"\"\n",
    "        비식별화 함수는 주어진 사용자 ID의 앞부분을 '*'로 대체하여 비식별화합니다.\n",
    "\n",
    "        :param user_id: 비식별화할 사용자 ID\n",
    "        :param num_chars_to_anonymize: 비식별화할 문자 수\n",
    "        :return: 비식별화된 사용자 ID\n",
    "        \"\"\"\n",
    "        # 비식별화할 문자 수가 사용자 ID의 길이보다 길 경우, 전체 ID를 '*'로 대체\n",
    "        if num_chars_to_anonymize >= len(user_id):\n",
    "            num_chars_to_anonymize = len(user_id) - 1\n",
    "            return \"*\" * num_chars_to_anonymize\n",
    "\n",
    "        # 앞부분을 '*'로 대체하고 나머지 부분을 원본 ID에서 가져옴\n",
    "        anonymized_id = \"*\" * num_chars_to_anonymize + user_id[num_chars_to_anonymize:]\n",
    "\n",
    "        return anonymized_id\n",
    "    \n",
    "    # NOTE - choh(2024.04.05) - 12시간제를 24시간제로 변환\n",
    "    def process_time_to_24hr_format(self, date_obj, time_str):\n",
    "        \"\"\"\n",
    "        대화 내용중에 시간 표시가 '오전 12:23', '오후 11:23'과 같이 12시간제로 되어 있는 경우, \n",
    "        이를 24시간제로 변환합니다.\n",
    "        \n",
    "        :param date_obj: 대화 내용의 날짜 정보가 담긴 datetime 객체\n",
    "        :praam time_str: 대화 내용의 시간 정보가 담긴 문자열\n",
    "        :return: 24시간제로 변환된 datetime 객체\n",
    "        \"\"\"\n",
    "        \n",
    "        # '오전/오후' 부분과 시간 부분을 분리합니다.\n",
    "        period, time_part = time_str.split(' ', 1)\n",
    "        \n",
    "        # 시간 부분을 시와 분으로 다시 분리합니다.\n",
    "        hour, minute = map(int, time_part.split(':'))\n",
    "        \n",
    "        # '오후'인 경우 12를 더하되, '오후 12시'는 제외합니다.\n",
    "        if period == '오후' and hour != 12:\n",
    "            hour += 12\n",
    "        # '오전 12시'는 0시로 처리합니다.\n",
    "        elif period == '오전' and hour == 12:\n",
    "            hour = 0\n",
    "        \n",
    "        # date_obj과 결합하여 최종 datetime 객체를 생성합니다.\n",
    "        # 여기서 datetime 함수는 위에서 임포트한 datetime 클래스를 사용합니다.\n",
    "        combined_datetime = datetime(date_obj.year, date_obj.month, date_obj.day, hour, minute)\n",
    "        \n",
    "        # pandas의 to_datetime 함수를 사용하여 pandas.Timestamp 객체로 변환합니다.\n",
    "        return pd.to_datetime(combined_datetime)\n",
    "    \n",
    "    # NOTE - choh(2024.04.05) - 대화목록의 날짜 변환 부분을 파싱\n",
    "    def process_date(self, line: str) -> tuple:\n",
    "        \"\"\"\n",
    "        -------- 2024년 4월 5일 화요일 -------- 형태의 날짜를 파싱하고,\n",
    "        파싱 성공 여부와 함께 파싱된 날짜 또는 원래 문자열을 반환합니다.\n",
    "        \n",
    "        :param line: 날짜 문자열\n",
    "        :return: (파싱 성공 여부, 파싱된 날짜 또는 원래 문자열)\n",
    "        \"\"\"\n",
    "        # -------- 2024년 4월 5일 화요일 -------- 날짜가 이상태임\n",
    "        date_match = re.match(r'[-]+ (\\d+년 \\d+월 \\d+일) [^\\d]+', line)\n",
    "        if date_match:\n",
    "            # 2024년 4월 5일, 형태의 날짜 추출\n",
    "            date_pattern = re.compile(r'(\\d+)년 (\\d+)월 (\\d+)일')\n",
    "            match = date_pattern.search(date_match.group(1))\n",
    "            if match:\n",
    "                year, month, day = map(int, match.groups())\n",
    "                return (True, pd.to_datetime(f\"{year}-{month}-{day}\"))\n",
    "        return (False, line)\n",
    "\n",
    "    # NOTE - choh(2024.04.05) - __read_file을 테스트 하기 위한 wrapper 함수\n",
    "    def _read_file_test(self, csvfile) -> Iterator[Document]:\n",
    "        \"\"\"테스트를 위한 래퍼 함수\"\"\"\n",
    "        return self.__read_file(csvfile)\n",
    "    \n",
    "    def __read_file(self, csvfile) -> Iterator[Document]:\n",
    "        # NOTE - choh(2024.04.05) - TXT 형태의 대화 메세지 사전 처리\n",
    "        if self.file_suffix == \".txt\":\n",
    "            \n",
    "            # 전날 날짜 변수 초기화\n",
    "            temp_date = None\n",
    "            i = 0 # 행 번호\n",
    "            for line in csvfile:\n",
    "                \n",
    "                # 이번 줄이 날짜가 맞으면 is_parsed=True, result는 날짜\n",
    "                is_parsed, result = self.process_date(line)\n",
    "                \n",
    "                # 파싱한 문자열이 날짜 패턴에 맞으면, 날짜를 저장\n",
    "                if is_parsed:\n",
    "                    temp_date = result\n",
    "                \n",
    "                # 날짜가 아니면, 체팅이기 때문에, 체팅을 패턴 매칭\n",
    "                else:\n",
    "                    # 초기값 설정\n",
    "                    user = None\n",
    "                    time_12hr = None\n",
    "                    message = None\n",
    "\n",
    "                    # 대화 패턴 찾기\n",
    "                    conversation_match = re.match(r'\\[([^\\]]+)\\] \\[([^\\]]+)\\] (.+)', line)\n",
    "                    if conversation_match:\n",
    "                        user_real = conversation_match.group(1)\n",
    "                        time_12hr = conversation_match.group(2)\n",
    "                        message = conversation_match.group(3).strip()\n",
    "                        \n",
    "                        # 시간을 24시간제로 변환                        \n",
    "                        date = self.process_time_to_24hr_format(temp_date, time_12hr)\n",
    "                        # 사용자 ID 비식별화\n",
    "                        user = self.anonymize_user_id(user_real)\n",
    "                        \n",
    "                        content = f'\"User: {user}, Message: {message}'\n",
    "                        \n",
    "                        metadata = {\n",
    "                            \"date\":  date.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                            \"year\": date.year,\n",
    "                            \"month\": date.month,\n",
    "                            \"day\": date.day,\n",
    "                            \"user\": user,\n",
    "                            \"row\": i,\n",
    "                            \"source\": str(self.file_path),\n",
    "                        }\n",
    "                        i += 1 # 행 번호 증가\n",
    "                        yield Document(page_content=content, metadata=metadata)\n",
    "       \n",
    "        \n",
    "        # NOTE - choh(2024.04.05) - 기존 코드, csv 파일인 경우\n",
    "        else:\n",
    "            df = pd.read_csv(csvfile)\n",
    "            df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "            df[\"Date_strf\"] = df[\"Date\"].dt.strftime(\"%Y-%m-%d %H:%M:%S\").astype(str)\n",
    "            for i, row in df.iterrows():\n",
    "                date = row[\"Date\"]\n",
    "                user = self.anonymize_user_id(row[\"User\"])\n",
    "                content = f'\"User: {user}, Message: {row[\"Message\"]}'\n",
    "\n",
    "                metadata = {\n",
    "                    \"date\": row[\"Date_strf\"],\n",
    "                    \"year\": date.year,\n",
    "                    \"month\": date.month,\n",
    "                    \"day\": date.day,\n",
    "                    \"user\": user,\n",
    "                    \"row\": i,\n",
    "                    \"source\": str(self.file_path),\n",
    "                }\n",
    "                yield Document(page_content=content, metadata=metadata)\n",
    "\n",
    "    def lazy_load(self) -> Iterator[Document]:\n",
    "        try:\n",
    "            with open(self.file_path, newline=\"\", encoding=self.encoding) as csvfile:\n",
    "                yield from self.__read_file(csvfile)\n",
    "      \n",
    "        except UnicodeDecodeError as e:\n",
    "            if self.autodetect_encoding:\n",
    "                detected_encodings = detect_file_encodings(self.file_path)\n",
    "                for encoding in detected_encodings:\n",
    "                    try:\n",
    "                        with open(\n",
    "                            self.file_path, newline=\"\", encoding=encoding.encoding\n",
    "                        ) as csvfile:\n",
    "                            yield from self.__read_file(csvfile)\n",
    "                            break\n",
    "                    except UnicodeDecodeError:\n",
    "                        continue\n",
    "            else:\n",
    "                raise RuntimeError(f\"Error loading {self.file_path}\") from e\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error loading {self.file_path}\") from e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Y3TWw12cSqRw"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n",
      "/home/j-i11b104/.local/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n",
      "/home/j-i11b104/.local/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "# Build a sample vectorDB\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "import kakaotalk_loader as kakao\n",
    "\n",
    "file_path = \"KakaoTalk_group.txt\"\n",
    "file_suffix = \".txt\"  # Change to \".csv\" if you're using a CSV file\n",
    "loader = kakao.KaKaoTalkLoader(file_path, file_suffix, encoding=\"utf8\")\n",
    "\n",
    "# Load blog post\n",
    "loader = WebBaseLoader(\"https://corin-e.tistory.com/entry/1-%EC%82%BC%EC%84%B1-%EC%B2%AD%EB%85%84-SW-%EC%95%84%EC%B9%B4%EB%8D%B0%EB%AF%B8%EC%8B%B8%ED%94%BC-SSAFY-%EC%82%BC%EC%88%98-%ED%9B%84%EA%B8%B0\")\n",
    "data = loader.load()\n",
    "\n",
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=30)\n",
    "splits = text_splitter.split_documents(data)\n",
    "\n",
    "# VectorDB\n",
    "model_name = \"jhgan/ko-sbert-nli\"\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "ko_embedding = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "# sk-proj-5g3W1SVHGnmBB7Y2HEMCT3BlbkFJUSvDENpVaFEdRNwZHWlH\n",
    "# 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\n",
    "open_key = \"sk-proj-5g3W1SVHGnmBB7Y2HEMCT3BlbkFJUSvDENpVaFEdRNwZHWlH\"\n",
    "vectordb = Chroma.from_documents(documents=splits, embedding=ko_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "epoTy2FNSuqz"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/j-i11b104/.local/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "question = \"얼마의 유동성을 투입하기로 했어?\"\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, openai_api_key = open_key)\n",
    "retriever_from_llm = MultiQueryRetriever.from_llm(\n",
    "    retriever=vectordb.as_retriever(), llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "eU6X_HctSumR"
   },
   "outputs": [],
   "source": [
    "# Set logging for the queries\n",
    "import logging\n",
    "\n",
    "logging.basicConfig()\n",
    "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "pXYmox2MSuZ-"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/j-i11b104/.local/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "INFO:langchain.retrievers.multi_query:Generated queries: ['1. 투입할 유동성 자금은 얼마로 결정했나요?', '2. 얼마의 자금을 투자할 계획이신가요?', '3. 투입할 자금 규모는 어느 정도로 정했나요?']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_docs = retriever_from_llm.get_relevant_documents(query=question)\n",
    "len(unique_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "MIz2l4OaS75w"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'language': 'ko', 'source': 'https://n.news.naver.com/mnews/article/001/0014837066?rc=N&ntype=RANKING', 'title': '정부, 위메프·티몬 사태에 최소 5천600억 유동성 투입'}, page_content='정부, ‘티메프’ 피해 中企·소상공인에 5600억원 지원\\n긴급경영안정자금 2000억원, 신보·기은 협약프로그램 3000억원 등 포함 정부가 위메프·티몬의 판매 대금 미정산 사태로 판매대금을 받지 못해 어려움을 겪고 있는 중소기업·소상공인을 위해 5600억원 이상의 유동성을\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n결제대행사 결국 티메프 先환불…혼란 진정될까\\n티몬·위메프(티메프) 미정산 사태와 관련해 전자지급결제대행사(PG사)들이 소비자들에게 선 환불을 하기로 결정하면서 당분간 혼란은 일단락될 전망이다. 29일 금융권에 따르면 대형 PG사인 NHN KCP, 다날, 토스페\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n박성웅, 다리 \\'이 부분\\' 끊어져 절뚝....치료 어렵다는데, 무슨 일?\\n배우 박성웅이 촬영 중 햄스트링 부상을 당했다. 최근 박성웅은 영화 \\'필사의 추격\\' 제작보고회에서 햄스트링이 파열된 사실을 고백했다. 박성웅은 \"겨울에 촬영하다가 햄스트링이 뚝 끊어져 절뚝거리면서 처절하게 찍었다\"며'),\n",
       " Document(metadata={'language': 'ko', 'source': 'https://n.news.naver.com/mnews/article/001/0014837066?rc=N&ntype=RANKING', 'title': '정부, 위메프·티몬 사태에 최소 5천600억 유동성 투입'}, page_content=\"위메프·티몬[연합뉴스 자료사진]    (세종=연합뉴스) 이준서 기자 = 정부가 위메프·티몬의 판매대금 미정산 사태와 관련, 최소 5천600억원의 유동성을 즉시 투입하기로 했다.    정부는 29일 오전 정부서울청사에서 김범석 기획재정부 1차관 주재로 관계부처 태스크포스(TF) 2차 회의를 열어 이런 내용의 '위메프·티몬 사태 대응방안'을 발표했다.    회의에는 기재부를 비롯해 금융위원회, 금융감독원, 공정거래위원회, 중소벤처기업부, 산업통상자원부, 국무조정실, 국토교통부, 문화체육관광부 당국자들이 참석했다.    우선 중소기업·소상공인들이 판매대금을 받지 못해 어려움을 겪는 상황을 해소하기 위해 중소벤처기업진흥공단·소상공인시장진흥공단을 통한 긴급경영안정자금 2천억원, 신용보증기금·기술보증기금 협약프로그램으로 최소 3천억원의 유동성을 각각 지원하기로 했다. 여행사 이차보전(이자차액 보상)에도 600억원(대출규모) 한도로 지원한다.    이와 함께 피해기업의 대출·보증 만기를 최대\"),\n",
       " Document(metadata={'language': 'ko', 'source': 'https://n.news.naver.com/mnews/article/001/0014837066?rc=N&ntype=RANKING', 'title': '정부, 위메프·티몬 사태에 최소 5천600억 유동성 투입'}, page_content='기간한정무료\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t티메프 사태, 여파는 어디까지?\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\n\\n\\nBYTE+\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n기간한정무료\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t기초 편 - VC(Venture Capital)가 돈을 버는 원리\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\n\\n\\n인문학으로 투자하다.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t잇따른 ETF 명칭 변경, 간판 바꾼다고 달라질까요?\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\n\\n\\nthebellstock\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t[07.24 수 / 축구] 뉴캐슬 제츠 vs 웨스턴 유나이티드 승부 예측 프리뷰\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\n\\n\\n스포츠폴리오\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t씬 레드 라인\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\n\\n\\n소소한 책/영화/만화/일상\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t[데일리 픽] 네바다 주, VPP 진흥을 위한 도전\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\n\\n\\n제로 에너지 시대가 온다'),\n",
       " Document(metadata={'language': 'ko', 'source': 'https://n.news.naver.com/mnews/article/001/0014837066?rc=N&ntype=RANKING', 'title': '정부, 위메프·티몬 사태에 최소 5천600억 유동성 투입'}, page_content='SNS 보내기\\n\\n\\n\\n인쇄하기\\n\\n\\n\\n\\n\\n\\n\\n\\n경영안정자금 2천억·보증기금 협약 3천억·여행사 이차보전 600억기재차관 \"최종책임은 위메프·티몬\"…위법사항 합동점검반 운영\"현재 미정산 2천100억 추산…정산기일 도래분 고려하면 피해 커질 것\"'),\n",
       " Document(metadata={'language': 'ko', 'source': 'https://n.news.naver.com/mnews/article/001/0014837066?rc=N&ntype=RANKING', 'title': '정부, 위메프·티몬 사태에 최소 5천600억 유동성 투입'}, page_content='위메프, 티몬 판매대금 미정산 관련 관계부처 TF회의(서울=연합뉴스) 최재구 기자 = 기획재정부 김범석 1차관이 29일 서울 종로구 정부서울청사에서 열린 위메프, 티몬 판매대금 미정산 관련 관계부처 TF회의에서 발언하고 있다. 2024.7.29 jjaeck9@yna.co.kr    현재까지 업체로부터 파악된 미정산 금액은 약 2천100억원으로 추산되지만, 향후 정산기일이 다가오는 거래분까지 감안하면 피해 규모는 커질 것이라는 게 정부 판단이다.    김범석 차관은 \"이번 사태의 최종적인 책임은 약속한 판매대금을 지급하지 않은 위메프·티몬에 있다\"면서도 \"정부로서는 선량한 소비자와 판매자가 입은 피해를 지켜볼 수 없기에 가용자원을 최대한 동원해 피해가 최소화되도록 지원하겠다\"고 말했다.    기본적으로는 위메프와 티몬의 과실책임이라는 점에서 책임 있는 해결책을 재차 촉구하는 동시에 위법 사항을 점검한다는 방침이다.    김 차관은 \"금감원·공정위를 중심으로 합동점검반을 운영해')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "digNJMyRtM3k"
   },
   "source": [
    "## 기본 Parent-document Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "id": "tZOk-3LOnb-0"
   },
   "outputs": [],
   "source": [
    "from langchain.retrievers import ParentDocumentRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "id": "f0ONC3wZn62e"
   },
   "outputs": [],
   "source": [
    "from langchain.storage import InMemoryStore\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "id": "VpJhZWThn7gG"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Maybe you meant '==' or ':=' instead of '='? (2736647683.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[123], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    loader = TextLoader('KakaoTalk_group.txt')\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax. Maybe you meant '==' or ':=' instead of '='?\n"
     ]
    }
   ],
   "source": [
    "loaders = [\n",
    "    loader = TextLoader('KakaoTalk_group.txt')\n",
    "    # PyPDFLoader(\"/content/drive/MyDrive/강의 자료/[복지이슈 FOCUS 15ȣ] 경기도 극저신용대출심사모형 개발을 위한 국내 신용정보 활용가능성 탐색.pdf\"),\n",
    "    # PyPDFLoader(\"/content/drive/MyDrive/강의 자료/[이슈리포트 2022-2호] 혁신성장 정책금융 동향.pdf\"),\n",
    "]\n",
    "docs = []\n",
    "for loader in loaders:\n",
    "    docs.extend(loader.load_and_split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "id": "KBb-1ym5pKRG"
   },
   "outputs": [],
   "source": [
    "model_name = \"jhgan/ko-sbert-nli\"\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "ko_embedding = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "id": "fyixQa5SpHOW"
   },
   "outputs": [],
   "source": [
    "# This text splitter is used to create the child documents\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=200)\n",
    "# The vectorstore to use to index the child chunks\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"full_documents\", embedding_function=ko_embedding\n",
    ")\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryStore()\n",
    "retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "id": "IyVXzfq0pHME"
   },
   "outputs": [],
   "source": [
    "retriever.add_documents(docs, ids=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "id": "L8z2B2rKrAv3"
   },
   "outputs": [],
   "source": [
    "sub_docs = vectorstore.similarity_search(\"7월 29일 대화 내용\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "id": "_axFoMvnrA-h"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "글 길이: 46\n",
      "\n",
      "\n",
      "1반 에이스 님과 카카오톡 대화\n",
      "저장한 날짜 : 2024-07-29 13:47:37\n"
     ]
    }
   ],
   "source": [
    "print(\"글 길이: {}\\n\\n\".format(len(sub_docs[0].page_content)))\n",
    "print(sub_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "id": "W68G7ITorThj"
   },
   "outputs": [],
   "source": [
    "retrieved_docs = retriever.get_relevant_documents(\"7월 29일 대화 내용\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "id": "Uzl334J8rXEQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "글 길이: 46\n",
      "\n",
      "\n",
      "1반 에이스 님과 카카오톡 대화\n",
      "저장한 날짜 : 2024-07-29 13:47:37\n"
     ]
    }
   ],
   "source": [
    "print(\"글 길이: {}\\n\\n\".format(len(retrieved_docs[0].page_content)))\n",
    "print(retrieved_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AK752NU_tHjb"
   },
   "source": [
    "## 본문의 Full_chunk가 너무 길때"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "id": "q64Iw5Wbrd94"
   },
   "outputs": [],
   "source": [
    "# This text splitter is used to create the parent documents\n",
    "parent_splitter = RecursiveCharacterTextSplitter(chunk_size=800)\n",
    "# This text splitter is used to create the child documents\n",
    "# It should create documents smaller than the parent\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=200)\n",
    "# The vectorstore to use to index the child chunks\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"split_parents\", embedding_function=ko_embedding\n",
    ")\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryStore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "id": "1TVbBTmgr_CA"
   },
   "outputs": [],
   "source": [
    "retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    "    parent_splitter=parent_splitter,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "id": "K9bPu58hsAfP"
   },
   "outputs": [],
   "source": [
    "retriever.add_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "id": "KLvoTdeksUne"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(store.yield_keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "id": "K0Te96C7sVxA"
   },
   "outputs": [],
   "source": [
    "sub_docs = vectorstore.similarity_search(\"7월 29일 대화 내용\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "id": "qJ6Qb_E_sZCb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1반 에이스 님과 카카오톡 대화\n",
      "저장한 날짜 : 2024-07-29 13:47:37\n"
     ]
    }
   ],
   "source": [
    "print(sub_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "id": "GXioc_E3so_U"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sub_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "id": "bL6F-aBJsZyv"
   },
   "outputs": [],
   "source": [
    "retrieved_docs = retriever.get_relevant_documents(\"7월 29일 대화 내용\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "id": "EnhD6dY_sfKq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1반 에이스 님과 카카오톡 대화\n",
      "저장한 날짜 : 2024-07-29 13:47:37\n"
     ]
    }
   ],
   "source": [
    "print(retrieved_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "id": "UhxrZ1zAscVP"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(retrieved_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FoTL9eBctWva"
   },
   "source": [
    "## Self-querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "_DmHA3cgtZcJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting lark\n",
      "  Downloading lark-1.1.9-py3-none-any.whl.metadata (1.9 kB)\n",
      "Downloading lark-1.1.9-py3-none-any.whl (111 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.7/111.7 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: lark\n",
      "Successfully installed lark-1.1.9\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install lark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7TJLhLfQthCt"
   },
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "docs = [\n",
    "    Document(\n",
    "        page_content=\"A bunch of scientists bring back dinosaurs and mayhem breaks loose\",\n",
    "        metadata={\"year\": 1993, \"rating\": 7.7, \"genre\": \"science fiction\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Leo DiCaprio gets lost in a dream within a dream within a dream within a ...\",\n",
    "        metadata={\"year\": 2010, \"director\": \"Christopher Nolan\", \"rating\": 8.2},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea\",\n",
    "        metadata={\"year\": 2006, \"director\": \"Satoshi Kon\", \"rating\": 8.6},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"A bunch of normal-sized women are supremely wholesome and some men pine after them\",\n",
    "        metadata={\"year\": 2019, \"director\": \"Greta Gerwig\", \"rating\": 8.3},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Toys come alive and have a blast doing so\",\n",
    "        metadata={\"year\": 1995, \"genre\": \"animated\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Three men walk into the Zone, three men walk out of the Zone\",\n",
    "        metadata={\n",
    "            \"year\": 1979,\n",
    "            \"director\": \"Andrei Tarkovsky\",\n",
    "            \"genre\": \"thriller\",\n",
    "            \"rating\": 9.9,\n",
    "        },\n",
    "    ),\n",
    "]\n",
    "vectorstore = Chroma.from_documents(docs, ko_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9x2PsJ9e7tuH"
   },
   "outputs": [],
   "source": [
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "metadata_field_info = [\n",
    "    AttributeInfo(\n",
    "        name=\"genre\",\n",
    "        description=\"The genre of the movie. One of ['science fiction', 'comedy', 'drama', 'thriller', 'romance', 'action', 'animated']\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"year\",\n",
    "        description=\"The year the movie was released\",\n",
    "        type=\"integer\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"director\",\n",
    "        description=\"The name of the movie director\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"rating\", description=\"A 1-10 rating for the movie\", type=\"float\"\n",
    "    ),\n",
    "]\n",
    "document_content_description = \"Brief summary of a movie\"\n",
    "llm = ChatOpenAI(temperature=0, openai_api_key = \"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\")\n",
    "retriever = SelfQueryRetriever.from_llm(\n",
    "    llm,\n",
    "    vectorstore,\n",
    "    document_content_description,\n",
    "    metadata_field_info,\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "foYY4IEA74Dr"
   },
   "outputs": [],
   "source": [
    "retriever.get_relevant_documents(\"what are some movies rated higher than 8.5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zQP9iztA8ApB"
   },
   "source": [
    "## Time-weighted vector store Retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OKEps8vK8GIm"
   },
   "source": [
    "Scoring 방법 = *semantic_similarity + (1.0 - decay_rate) ^ hours_passed*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "zKL8WJ_P8UEG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q faiss-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "uLG48BJY8AEW"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'faiss'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datetime, timedelta\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfaiss\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocstore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InMemoryDocstore\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mretrievers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TimeWeightedVectorStoreRetriever\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'faiss'"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "import faiss\n",
    "from langchain.docstore import InMemoryDocstore\n",
    "from langchain.retrievers import TimeWeightedVectorStoreRetriever\n",
    "from langchain.schema import Document\n",
    "from langchain_community.vectorstores import FAISS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lv5TtUnK8Mml"
   },
   "outputs": [],
   "source": [
    "# Initialize the vectorstore as empty\n",
    "embedding_size = 768\n",
    "index = faiss.IndexFlatL2(embedding_size)\n",
    "vectorstore = FAISS(ko_embedding, index, InMemoryDocstore({}), {})\n",
    "retriever = TimeWeightedVectorStoreRetriever(\n",
    "    vectorstore=vectorstore, decay_rate=0.99, k=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vL-HP-9L8OIq"
   },
   "outputs": [],
   "source": [
    "yesterday = datetime.now() - timedelta(days=1)\n",
    "retriever.add_documents(\n",
    "    [Document(page_content=\"영어는 훌륭합니다.\", metadata={\"last_accessed_at\": yesterday})]\n",
    ")\n",
    "retriever.add_documents([Document(page_content=\"한국어는 훌륭합니다\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q6KCbXXV_d4F"
   },
   "outputs": [],
   "source": [
    "# \"Hello World\" is returned first because it is most salient, and the decay rate is close to 0., meaning it's still recent enough\n",
    "retriever.get_relevant_documents(\"영어가 좋아요\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sjtkFgOGfemK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ltRw8HCmmECS"
   },
   "source": [
    "## Ensemble Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "QN9Qk5_LkeQH"
   },
   "outputs": [],
   "source": [
    "!pip install -q langchain pypdf sentence-transformers chromadb langchain-openai faiss-gpu --upgrade --quiet  rank_bm25 > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "h7HV_yPpmKdX"
   },
   "outputs": [],
   "source": [
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader('KakaoTalk_group.txt')\n",
    "data = loader.load()\n",
    "\n",
    "print(type(data))\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'KakaoTalk_group.txt'}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader('KakaoTalk_group.txt')\n",
    "data = loader.load()\n",
    "\n",
    "print(type(data))\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "OYhREH6zmMJp"
   },
   "outputs": [],
   "source": [
    "model_name = \"jhgan/ko-sbert-nli\"\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "ko_embedding = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "15n17ed7meJI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<langchain_community.document_loaders.text.TextLoader object at 0x7f2a717d6920>]\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loaders = [\n",
    "    # PyPDFLoader(\"./first.pdf\"),\n",
    "    # PyPDFLoader(\"./fsecond.pdf\"),\n",
    "    TextLoader('KakaoTalk_group.txt')\n",
    "]\n",
    "print(loaders)\n",
    "docs = []\n",
    "for loader in loaders:\n",
    "    docs.extend(loader.load_and_split())\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=50)\n",
    "texts = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "ck2cUaLX4pa3"
   },
   "outputs": [],
   "source": [
    "# initialize the bm25 retriever and faiss retriever\n",
    "bm25_retriever = BM25Retriever.from_documents(texts)\n",
    "bm25_retriever.k = 2\n",
    "\n",
    "\n",
    "\n",
    "embedding = ko_embedding\n",
    "faiss_vectorstore = FAISS.from_documents(texts, ko_embedding)\n",
    "faiss_retriever = faiss_vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "# initialize the ensemble retriever\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[bm25_retriever, faiss_retriever], weights=[0.5, 0.5]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "JcBSDvy3mmlx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'KakaoTalk_group.txt'}\n",
      ":\n",
      "[김문희] [오후 11:23] 204조는 진짜 아무도 모르겟음\n",
      "--------------- 2024년 7월 29일 월요일 ---------------\n",
      "[진기] [오전 12:34] ㅋㅋㅋㅋㅋ\n",
      "[진기] [오전 12:35] 방금까지 도커를 했지만 완료하지 못해 내일까지 완료하겠습니다...;\n",
      "[진기] [오전 12:35] 204조는 한 번 찾아볼까요?\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'source': 'KakaoTalk_group.txt'}\n",
      ":\n",
      "[김문희] [오전 8:51] https://youtu.be/T59SXQlneLY?si=bdY01AHKYja83Z6v\n",
      "[진기] [오후 12:58] 예본아 카페야\n",
      "[박예본] [오후 12:59] ㅇㅋㅇㅋ\n",
      "[진현지] [오후 4:01] 파일: 아이디어 회의.pdf\n",
      "--------------- 2024년 7월 14일 일요일 ---------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'source': 'KakaoTalk_group.txt'}\n",
      ":\n",
      "--------------- 2024년 7월 10일 수요일 ---------------\n",
      "[김문희] [오전 7:02] @all\n",
      "## 셔틀버스안내\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'source': 'KakaoTalk_group.txt'}\n",
      ":\n",
      "1반 에이스 님과 카카오톡 대화\n",
      "저장한 날짜 : 2024-07-29 13:47:37\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# docs = ensemble_retriever.invoke(\"혁신정책금융과 극저신용대출모형의 차이\")\n",
    "docs = ensemble_retriever.invoke(\"7월 29일 대화내용\")\n",
    "for i in docs:\n",
    "\n",
    "  print(i.metadata)\n",
    "  print(\":\")\n",
    "  print(i.page_content)\n",
    "  print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "c0Jhox_l6qe2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'KakaoTalk_group.txt'}\n",
      ":\n",
      "[김문희] [오전 8:51] https://youtu.be/T59SXQlneLY?si=bdY01AHKYja83Z6v\n",
      "[진기] [오후 12:58] 예본아 카페야\n",
      "[박예본] [오후 12:59] ㅇㅋㅇㅋ\n",
      "[진현지] [오후 4:01] 파일: 아이디어 회의.pdf\n",
      "--------------- 2024년 7월 14일 일요일 ---------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'source': 'KakaoTalk_group.txt'}\n",
      ":\n",
      "1반 에이스 님과 카카오톡 대화\n",
      "저장한 날짜 : 2024-07-29 13:47:37\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'source': 'KakaoTalk_group.txt'}\n",
      ":\n",
      "[박예본] [오후 2:28] 파일: ion.pptx\n",
      "[김문희] [오후 5:22] https://padlet.com/dudgml1531/ssafy11-4f-s9vaadxudtxeusc4\n",
      "--------------- 2024년 7월 5일 금요일 ---------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'source': 'KakaoTalk_group.txt'}\n",
      ":\n",
      "--------------- 2024년 7월 25일 목요일 ---------------\n",
      "[김문희] [오전 8:47] https://youtu.be/dBDkYofMUs4?si=mmDs4MSRbBSvEkMJ\n",
      "[진기] [오전 8:56] https://youtu.be/-m_Kow1fDMo\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "faiss_vectorstore = FAISS.from_documents(texts, ko_embedding)\n",
    "faiss_retriever = faiss_vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "docs = faiss_retriever.invoke(\"7월 29일 대화내용\")\n",
    "for i in docs:\n",
    "\n",
    "  print(i.metadata)\n",
    "  print(\":\")\n",
    "  print(i.page_content)\n",
    "  print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "28vloMiumuY_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7월 29일에 김문희와 진기가 대화한 내용은 다음과 같습니다:\n",
      "\n",
      "[김문희] [오후 11:23] 204조는 진짜 아무도 모르겟음\n",
      "[진기] [오후 11:26] 이모티콘\n",
      "[진기] [오후 11:36] 문희님이 가자면 가야조.. 저희가 무슨 선택권이 ㅜ\n",
      "[김문희] [오후 11:36] 네 ?\n",
      "[김문희] [오후 11:36] 이모티콘\n",
      "\n",
      "이 날짜에 대화한 내용은 204조에 대한 언급과 함께 진기와 김문희가 대화를 주고받았습니다.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = 'sk-proj-5g3W1SVHGnmBB7Y2HEMCT3BlbkFJUSvDENpVaFEdRNwZHWlH'\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "openai = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature = 0)\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm = openai,\n",
    "                                 chain_type = \"stuff\",\n",
    "                                 retriever = ensemble_retriever,\n",
    "                                 return_source_documents = True)\n",
    "\n",
    "query = \"7월 29일 카카오톡 내용\"\n",
    "result = qa(query)\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "LxPfaF0L0c2I"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'KakaoTalk_group.txt'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "1반 에이스 님과 카카오톡 대화\n",
      "저장한 날짜 : 2024-07-29 13:47:37\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'source': 'KakaoTalk_group.txt'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[김문희] [오후 11:23] 204조는 진짜 아무도 모르겟음\n",
      "--------------- 2024년 7월 29일 월요일 ---------------\n",
      "[진기] [오전 12:34] ㅋㅋㅋㅋㅋ\n",
      "[진기] [오전 12:35] 방금까지 도커를 했지만 완료하지 못해 내일까지 완료하겠습니다...;\n",
      "[진기] [오전 12:35] 204조는 한 번 찾아볼까요?\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'source': 'KakaoTalk_group.txt'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[김문희] [오후 11:26] 이모티콘\n",
      "[진기] [오후 11:36] 문희님이 가자면 가야조.. 저희가 무슨 선택권이 ㅜ\n",
      "[김문희] [오후 11:36] 네 ?\n",
      "[김문희] [오후 11:36] 이모티콘\n",
      "--------------- 2024년 7월 8일 월요일 ---------------\n",
      "[진현지] [오전 12:07] 저도 완전 좋아요~!~!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in result['source_documents']:\n",
    "  print(i.metadata)\n",
    "  print(\"-\"*100)\n",
    "  print(i.page_content)\n",
    "  print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "bxftciR52lYW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "죄송합니다, 2024-07-28에 [박상필]이 보낸 메시지에 대한 정보는 제공되지 않았습니다. 해당 대화 내용이 없는 것 같습니다.\n"
     ]
    }
   ],
   "source": [
    "faiss_vectorstore = FAISS.from_documents(docs, ko_embedding)\n",
    "faiss_retriever = faiss_vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm = openai,\n",
    "                                 chain_type = \"stuff\",\n",
    "                                 retriever = faiss_retriever,\n",
    "                                 return_source_documents = True)\n",
    "\n",
    "query = \"2024-07-28 에 [박상필]이 보낸 메세지\"\n",
    "result = qa(query)\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "kId5kM9H3LxY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'KakaoTalk_group.txt'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[박예본] [오후 2:28] 파일: ion.pptx\n",
      "[김문희] [오후 5:22] https://padlet.com/dudgml1531/ssafy11-4f-s9vaadxudtxeusc4\n",
      "--------------- 2024년 7월 5일 금요일 ---------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'source': 'KakaoTalk_group.txt'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "--------------- 2024년 7월 25일 목요일 ---------------\n",
      "[김문희] [오전 8:47] https://youtu.be/dBDkYofMUs4?si=mmDs4MSRbBSvEkMJ\n",
      "[진기] [오전 8:56] https://youtu.be/-m_Kow1fDMo\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'source': 'KakaoTalk_group.txt'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[김문희] [오전 8:51] https://youtu.be/T59SXQlneLY?si=bdY01AHKYja83Z6v\n",
      "[진기] [오후 12:58] 예본아 카페야\n",
      "[박예본] [오후 12:59] ㅇㅋㅇㅋ\n",
      "[진현지] [오후 4:01] 파일: 아이디어 회의.pdf\n",
      "--------------- 2024년 7월 14일 일요일 ---------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'source': 'KakaoTalk_group.txt'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "1반 에이스 님과 카카오톡 대화\n",
      "저장한 날짜 : 2024-07-29 13:47:37\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in result['source_documents']:\n",
    "  print(i.metadata)\n",
    "  print(\"-\"*100)\n",
    "  print(i.page_content)\n",
    "  print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MXZ7dGLm3e8A"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-YinIRx6_--y"
   },
   "source": [
    "## Long Context Reorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "53bnfKrCAPxj"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain, StuffDocumentsChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.document_transformers import (\n",
    "    LongContextReorder,\n",
    ")\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "texts = [\n",
    "    \"바스켓볼은 훌륭한 스포츠입니다.\",\n",
    "    \"플라이 미 투 더 문은 제가 가장 좋아하는 노래 중 하나입니다.\",\n",
    "    \"셀틱스는 제가 가장 좋아하는 팀입니다.\",\n",
    "    \"보스턴 셀틱스에 관한 문서입니다.\", \"보스턴 셀틱스는 제가 가장 좋아하는 팀입니다.\",\n",
    "    \"저는 영화 보러 가는 것을 좋아해요\",\n",
    "    \"보스턴 셀틱스가 20점차로 이겼어요\",\n",
    "    \"이것은 그냥 임의의 텍스트입니다.\",\n",
    "    \"엘든 링은 지난 15 년 동안 최고의 게임 중 하나입니다.\",\n",
    "    \"L. 코넷은 최고의 셀틱스 선수 중 한 명입니다.\",\n",
    "    \"래리 버드는 상징적 인 NBA 선수였습니다.\",\n",
    "]\n",
    "\n",
    "# Create a retriever\n",
    "retriever = Chroma.from_texts(texts, embedding=ko_embedding).as_retriever(\n",
    "    search_kwargs={\"k\": 10}\n",
    ")\n",
    "query = \"셀틱스에 대해 어떤 이야기를 들려주시겠어요?\"\n",
    "\n",
    "# Get relevant documents ordered by relevance score\n",
    "docs = retriever.get_relevant_documents(query)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x_9sEUx0AX9h"
   },
   "outputs": [],
   "source": [
    "reordering = LongContextReorder()\n",
    "reordered_docs = reordering.transform_documents(docs)\n",
    "\n",
    "# Confirm that the 4 relevant documents are at beginning and end.\n",
    "reordered_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jxdYbGqgA4Sf"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain, StuffDocumentsChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = 'YOUR_API_KEY'\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "document_prompt = PromptTemplate(\n",
    "    input_variables=[\"page_content\"], template=\"{page_content}\"\n",
    ")\n",
    "\n",
    "template = \"\"\"Given this text extracts:\n",
    "-----\n",
    "{context}\n",
    "-----\n",
    "Please answer the following question:\n",
    "{query}\"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    template=template, input_variables=[\"context\", \"query\"]\n",
    ")\n",
    "openai = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature = 0)\n",
    "\n",
    "llm_chain = LLMChain(llm=openai, prompt=prompt)\n",
    "chain = StuffDocumentsChain(\n",
    "    llm_chain=llm_chain,\n",
    "    document_prompt=document_prompt,\n",
    "    document_variable_name=\"context\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cn0oLiy3D_WR"
   },
   "outputs": [],
   "source": [
    "reordered_result = chain.run(input_documents=reordered_docs, query=query)\n",
    "result = chain.run(input_documents=docs, query=query)\n",
    "\n",
    "print(reordered_result)\n",
    "print(\"-\"*100)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JaIjA6LWEdP2"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "_HBhuL23S_YR",
    "digNJMyRtM3k",
    "AK752NU_tHjb",
    "FoTL9eBctWva",
    "zQP9iztA8ApB"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "rag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
